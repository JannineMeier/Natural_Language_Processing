{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8ca764-4290-4bd1-ad09-d2fcc58ca83f",
   "metadata": {},
   "source": [
    "### Project 1: Word Embeddings\n",
    "This is the first project in NLP SW03 done by Jannine Meier. \n",
    "\n",
    "The project was run locally (in GPUHubllabservices).\n",
    "\n",
    "WandB: https://wandb.ai/nlp_janninemeier/Project1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f21dfc-e74c-4640-9f60-8bacd401cbd5",
   "metadata": {},
   "source": [
    "### Install libraries\n",
    "Installs all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839d899f-bff6-4412-93f6-d6e4c4ce55b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.11/site-packages (0.16.4)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.21.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (1.40.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim datasets torch transformers wandb nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e529ad9-3830-4e7d-a57d-1a3655abd4e9",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Imports all the necessary tools to handle data loading, preprocessing, model creation training, tracking experiments and analyzing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4ae00e-b14e-4796-b052-4aec507d599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PyTorch related imports (Machine Learning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Data handling and processing\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import Dict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import normalize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Word2Vec (Word embedding)\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Other utility libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f70227e-174c-4aad-ae25-000a88c7098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Run it on GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeadb35-bd57-4388-bddf-fe5c01fe5325",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Imports the specific subset winogrande_m data set from HuggingFace. \n",
    "\n",
    "#### Split the Dataset into Training, Validation, and Testing Sets\n",
    "\n",
    "- **`train_dataset`**: Extracts the training portion of the dataset. Training data is used to fit the machine learning model.\n",
    "- **`val_dataset`**: Extracts the validation portion of the dataset. Validation data is used to evaluate the model during the tuning of model hyperparameters.\n",
    "- **`test_dataset`**: Assigns the validation split of the dataset to `test_dataset`. This is a temporary workaround because the actual test set does not contain answer labels, which are necessary for evaluating the model's performance. This approach does not provide an unbiased estimate of the model's performance on truly unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c265b9e6-b337-4631-969f-a7a4ea8f5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face datasets\n",
    "winogrande_dataset = load_dataset('winogrande', 'winogrande_m')\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_dataset = winogrande_dataset['train']\n",
    "val_dataset = winogrande_dataset['validation']\n",
    "test_dataset = winogrande_dataset['validation'] # workaround for test data set with validation data set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794881b2-7e93-4ea3-843c-98e8504470f9",
   "metadata": {},
   "source": [
    "### Configuration of hyperparameters\n",
    "\n",
    "The configuration dictionary is a structured way to organize and manage the hyperparameters and preprocessing settings for this project. \n",
    "\n",
    "My wandb runs are labelled with hidden_dimension/learning_rate/num_epochs.\n",
    "\n",
    "#### Configuration Keys\n",
    "\n",
    "- **`hidden_dim`**: The dimensionality of the hidden layer(s) in the neural network. A higher number of dimensions can potentially capture more complex patterns in the data but may also lead to overfitting and increased computational cost. This was used for hyperparameter tuning.\n",
    "\n",
    "- **`output_dim`**: Dimensionality of the output layer of the neural network. For binary classification tasks this is typically set to `2`, indicating two possible outcomes (e.g., option1 or option2).\n",
    "\n",
    "- **`num_epochs`**: The number of complete passes through the training dataset. The goal is to choose a value that balances between underfitting and overfitting, as well as training time. Adjusting the number of epochs can impact the model's performance significantly. This was used for hyperparameter tuning.\n",
    "\n",
    "- **`learning_rate`**: Step size at each iteration while moving toward a minimum of the loss function. A smaller learning rate, ensures more precise updates, potentially leading to better convergence properties, but can also slow down the training process. This was used for hyperparameter tuning.\n",
    "\n",
    "- **`batch_size`**: Number of samples that will be propagated through the network in one forward/backward pass. A size of `32` is a common choice that balances the trade-off between computational efficiency and the stability of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9da472f-6541-4506-906b-9f3273873e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_dim\": 612,\n",
    "    \"output_dim\": 2,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"preprocessing\": {\n",
    "        \"lowercase\": True,              # Convert all text to lowercase\n",
    "        \"remove_punctuation\": True,     # Remove punctuation from text\n",
    "        \"use_stopwords\": False,         # Enable/Disable stopword removal\n",
    "        \"language\": \"english\",          # Language for stopword removal (if enabled)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698cbf0-adbc-4e64-bab1-9848f21a848e",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Text Normalization\n",
    "The process of converting text into a more uniform format. Here it normalizes a sentence by optionally converting it to lowercase and removing punctuation.\n",
    "\n",
    "####  Stopword Removal\n",
    "Stopwords are commonly used words (such as \"the\", \"a\", \"an\", \"in\"). Removing these can help focus on the more meaningful words in sentences. However, the impact of stopword removal can vary depending on the task, so it's useful to make this configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af9a4e7-2829-4b9d-8587-e88ded0daec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(sentence: str, remove_punctuation: bool = True, lowercase: bool = True) -> str:\n",
    "    if lowercase:\n",
    "        sentence = sentence.lower()\n",
    "    if remove_punctuation:\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)  # Remove punctuation\n",
    "    return sentence\n",
    "\n",
    "def remove_stopwords(sentence: str, use_stopwords: bool = False, language: str = 'english') -> str:\n",
    "    if not use_stopwords:\n",
    "        return sentence\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_sentence = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def preprocess_text(sentence: str, config: Dict[str, any], stop_words: set = None) -> str:\n",
    "    # Apply text normalization\n",
    "    if config[\"preprocessing\"][\"lowercase\"]:\n",
    "        sentence = sentence.lower()\n",
    "    if config[\"preprocessing\"][\"remove_punctuation\"]:\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    \n",
    "    # Apply stopword removal if enabled and stop words are provided\n",
    "    if stop_words and config[\"preprocessing\"][\"use_stopwords\"]:\n",
    "        words = word_tokenize(sentence)\n",
    "        sentence = ' '.join([word for word in words if word not in stop_words])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c1dd4-690d-4a61-983d-9a2062f8126e",
   "metadata": {},
   "source": [
    "### Word Embeddings and Sentence Vectorization\n",
    "\n",
    "#### Load Pre-trained Word2Vec Embeddings\n",
    "\n",
    "The api loads the pre-trained model `word2vec-google-news-300`. This model is trained on the Google News dataset and contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c805e7-7de5-4f3b-a5af-47bcb665373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc957779-51e4-4c74-ac4b-fe7950c7be30",
   "metadata": {},
   "source": [
    "#### Convert Sentence to Vector\n",
    "\n",
    "The `sentence_to_avg_vector` function converts a given sentence into a vector by averaging the word vectors of the words contained in the sentence. This approach allows for the representation of the entire sentence in a fixed-size vector, which can then be used for further ML tasks. This method averages the semantics of all words, potentially diluting the impact of particularly salient words and ignoring the order of words.\n",
    "\n",
    "- **Process**:\n",
    "  1. **Tokenization**: The sentence is split into individual words.\n",
    "  2. **Vector Accumulation**: For each word in the sentence, if the word is present in the Word2Vec model, its vector is added to a list of vectors. Words not found in the Word2Vec model are ignored. This step assumes that the meaningful content of a sentence can be adequately represented by the words found in the Word2Vec vocabulary.\n",
    "  3. **Averaging**: Calculates the mean of these vectors along the zeroth axis (column-wise mean if the vectors are stacked as rows in a matrix). This results in a single vector that represents the average of all word vectors in the sentence.\n",
    "\n",
    "The return is the average vector. If no words in the sentence are found in the Word2Vec model, a zero vector of the same dimensionality as the Word2Vec vectors is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2d1d36-d549-4083-8437-d49e5d0a4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg_vector(sentence, word_vectors):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Initialize an empty list to store the vectors\n",
    "    vector_list = []\n",
    "\n",
    "    # Iterate over each word in the sentence\n",
    "    for word in words:\n",
    "        # Check if the word is in the Word2Vec model\n",
    "        if word in word_vectors.key_to_index:\n",
    "            # Add the word's vector to the list\n",
    "            vector_list.append(word_vectors[word])\n",
    "\n",
    "    # If the list is empty (no words found in the Word2Vec model), return a zero vector\n",
    "    if not vector_list:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "    # Compute the average vector and normalize it\n",
    "    avg_vector = np.mean(vector_list, axis=0)\n",
    "    avg_vector = normalize(avg_vector.reshape(1, -1))  # Reshape for sklearn's normalize function\n",
    "    return avg_vector.flatten()  # Flatten to convert back to 1D array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18013b36-6ab2-40fa-a39d-71e874bb8b1e",
   "metadata": {},
   "source": [
    "### WinograndeDataset Class\n",
    "\n",
    "The `WinograndeDataset` class is designed to work with the dataset in the context of PyTorch machine learning tasks, enabling it to be used seamlessly with PyTorch's `DataLoader` for efficient and easy batching, shuffling, and parallel data loading and evaluation.\n",
    "\n",
    "- **Process**:\n",
    "  1. **Sentence and Options Extraction**: For the given index, the method extracts the sentence, two options (option1 and option2), and the correct answer from the dataset.\n",
    "  2. **Sentence Replacement**: It creates two new sentences by replacing a placeholder `_` in the original sentence with each of the two options.\n",
    "  3. **Vectorization**: Each modified sentence is then converted to an average vector using the `sentence_to_avg_vector` function, leveraging the word embeddings to encode the sentence semantically.\n",
    "  4. **Stacking and Labeling**: The two vectors are stacked into a tensor, and a label is created based on the correct answer (+1 for option1, -1 for option2).\n",
    "\n",
    "The return is a tuple of the stacked sentence vectors and the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c2bc6d-0fe3-4383-b559-42aa725923a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WinograndeDataset(Dataset):\n",
    "    def __init__(self, data, word_vectors, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: a 'Dataset' object from the Hugging Face 'datasets' library.\n",
    "            word_vectors: pre-loaded Word2Vec model.\n",
    "            config: configuration dictionary with preprocessing settings.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.word_vectors = word_vectors\n",
    "        self.config = config\n",
    "        \n",
    "        # Preload stop words if needed for efficiency\n",
    "        if self.config[\"preprocessing\"][\"use_stopwords\"]:\n",
    "            self.stop_words = set(stopwords.words(self.config[\"preprocessing\"][\"language\"]))\n",
    "        else:\n",
    "            self.stop_words = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of data samples.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the idx-th data sample from the dataset.\n",
    "        \"\"\"\n",
    "        # Get the sentence and the options from the dataset\n",
    "        sample = self.data[idx]\n",
    "        sentence = sample['sentence']\n",
    "        option1 = sample['option1']\n",
    "        option2 = sample['option2']\n",
    "        answer = sample['answer']\n",
    "    \n",
    "        # Create two versions of the sentence, one with each option\n",
    "        sentence_option1 = sentence.replace('_', option1)\n",
    "        sentence_option2 = sentence.replace('_', option2)\n",
    "        \n",
    "        # Convert each sentence to an average vector\n",
    "        vector_option1 = sentence_to_avg_vector(sentence_option1, self.word_vectors)\n",
    "        vector_option2 = sentence_to_avg_vector(sentence_option2, self.word_vectors)\n",
    "        \n",
    "        # Stack the vectors and convert to a tensor\n",
    "        sentence_vectors = torch.stack([torch.tensor(vector_option1), torch.tensor(vector_option2)], dim=0)\n",
    "        \n",
    "        # The label is +1 if the answer is '1' (option1), -1 if the answer is '2' (option2)\n",
    "        label = torch.tensor(1 if answer == '1' else -1, dtype=torch.long)\n",
    "        \n",
    "        return sentence_vectors, label\n",
    "\n",
    "\n",
    "train_dataset = WinograndeDataset(winogrande_dataset['train'], word_vectors, config)\n",
    "val_dataset = WinograndeDataset(winogrande_dataset['validation'], word_vectors, config)\n",
    "test_dataset = WinograndeDataset(winogrande_dataset['validation'], word_vectors, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40922a42-1742-46c8-ba67-d9aee98ec95a",
   "metadata": {},
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "#### DataLoader for the Training Set\n",
    "\n",
    "Prepares the training dataset for the model training process. - **`shuffle`**: Set to `True` as huffling helps prevent the model from learning spurious correlations based on the order of the data which helps improving the generalization.\n",
    "\n",
    "#### DataLoader for the Validation Set\n",
    "\n",
    "Sets up the validation dataset for evaluating the model periodically during training. - **`shuffle`**: Set to `False` because shuffling is not necessary for validation as the order of data does not affect evaluation metrics.\n",
    "\n",
    "#### DataLoader for the Test Set\n",
    "\n",
    "Prepares the test dataset for the final evaluation of the model to measure the performance after training and hyperparameter tuning have been completed. - **`shuffle`**: Also set to `False`, for the same reasons as the validation DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3446ac-f120-42af-91dc-47fdfa1bbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Create the DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d0531-19b6-4259-9fa7-11836c5882cb",
   "metadata": {},
   "source": [
    "### ComparativeClassifier Neural Network Class\n",
    "\n",
    "A neural network architecture for comparative classification tasks, specifically for evaluating pairs of sentence vectors. Its primary objective is to compare two sentence vectors and determine which sentence is more likely to be the correct fill-in for a given context, leveraging the power of learned word embeddings and a simple feedforward neural network structure.\n",
    "\n",
    "#### Class Constructor (`__init__`)\n",
    "\n",
    "- **Network Architecture**:\n",
    "  - The network consists of a sequential model (`nn.Sequential`) with the following layers:\n",
    "    1. **Linear Layer**: Transforms the input vector to the hidden layer size (`input_size` to `hidden_size`).\n",
    "    2. **ReLU Activation**: Introduces non-linearity into the model, allowing it to learn complex patterns.\n",
    "    3. **Linear Layer**: Further transforms the data from the hidden layer size to a single scalar value (`hidden_size` to `1`). This output represents the \"score\" of a sentence vector, essentially estimating its likelihood of being the correct answer.\n",
    "\n",
    "#### Forward Pass (`forward`)\n",
    "\n",
    "- **Input**:\n",
    "  - `sentence_pairs`: A tensor containing pairs of sentence vectors. Each pair corresponds to two variations of a sentence, with one word replaced by different options (option1 and option2).\n",
    "\n",
    "- **Process**:\n",
    "  1. **Vector Separation**: The method separates the sentence pairs into two individual vectors, `vec1` and `vec2`, representing the first and second sentence vectors in each pair.\n",
    "  2. **Scoring**: Each vector is independently passed through the neural network to compute a score, indicating its likelihood of correctness.\n",
    "  3. **Comparison**: The scores of `vec2` are subtracted from `vec1`. A positive result implies a preference for `vec1` (indicating it is more likely to be correct), while a negative result suggests a preference for `vec2`.\n",
    "\n",
    "- **Output**:\n",
    "  - The output is a tensor of score differences for each pair of sentence vectors. This differential scoring mechanism enables the model to effectively compare the two sentences and make a decision based on which one has the higher score, thus determining which option fits the context better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd659baa-7ba0-42af-ae67-6278ccb8fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparativeClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ComparativeClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)  # Output a single score per sentence vector\n",
    "        )\n",
    "    \n",
    "    def forward(self, sentence_pairs):\n",
    "        vec1 = sentence_pairs[:, 0, :]  # First sentence vector in each pair\n",
    "        vec2 = sentence_pairs[:, 1, :]  # Second sentence vector in each pair\n",
    "        \n",
    "        # Pass both vectors through the network\n",
    "        score1 = self.network(vec1)\n",
    "        score2 = self.network(vec2)\n",
    "        \n",
    "        # Subtract score2 from score1; positive values indicate preference for vec1, negative for vec2\n",
    "        return score1 - score2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a85c1e-e141-485c-a01c-4e3451bf7d69",
   "metadata": {},
   "source": [
    "### Experiment Setup and Training \n",
    "\n",
    "#### Weights & Biases Initialization\n",
    "\n",
    "Initializes a new run in Weights & Biases. This step configures the experiment tracking, allowing for the monitoring and analysis of model performance across different runs. The configuration dictionary `config` contains all hyperparameters and settings for the experiment, facilitating easy adjustments and tracking.\n",
    "\n",
    "#### Loss Function and Optimizer\n",
    "\n",
    "Defines the loss criterion using `MarginRankingLoss` with a specified margin, suitable for ranking and comparison tasks in the classification model. An Adam optimizer is initialized to update the model's weights, with the learning rate set according to the Weights & Biases configuration.\n",
    "\n",
    "\n",
    "#### Training Loop\n",
    "\n",
    "Iterates over the dataset for a specified number of epochs, performing the following actions for each epoch:\n",
    "- Sets the model to training mode.\n",
    "- Iterates over the training dataset in batches, computing the loss, performing backpropagation, and updating the model weights accordingly.\n",
    "- Computes training accuracy based on the model's performance on the training data.\n",
    "- Logs training accuracy and loss metrics for each epoch to Weights & Biases for real-time tracking and analysis.\n",
    "\n",
    "#### Validation Loop\n",
    "\n",
    "Evaluates the model on a separate validation dataset after each training epoch to monitor performance on unseen data:\n",
    "- Sets the model to evaluation mode to disable gradient computations.\n",
    "- Computes validation accuracy by comparing the model's predictions against the true labels.\n",
    "- Updates and logs the best validation accuracy seen during training to Weights & Biases.\n",
    "\n",
    "#### Best Scores Tracking and Model Checkpointing\n",
    "\n",
    "Tracks the best validation accuracy during the training process and saves the final model checkpoint to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655215be-b1c9-4800-88ed-5816431d7d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjannine-meier\u001b[0m (\u001b[33mnlp_janninemeier\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NLP_Final/wandb/run-20240307_222336-lxvm3qn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_janninemeier/Project1/runs/lxvm3qn4' target=\"_blank\">tough-terrain-12</a></strong> to <a href='https://wandb.ai/nlp_janninemeier/Project1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_janninemeier/Project1' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_janninemeier/Project1/runs/lxvm3qn4' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project1/runs/lxvm3qn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.00107079, Training Accuracy: 41.44%, Validation Accuracy: 51.46%\n",
      "Epoch 2/100, Train Loss: 1.00040066, Training Accuracy: 43.32%, Validation Accuracy: 49.49%\n",
      "Epoch 3/100, Train Loss: 1.00025810, Training Accuracy: 44.84%, Validation Accuracy: 49.33%\n",
      "Epoch 4/100, Train Loss: 1.00024807, Training Accuracy: 44.06%, Validation Accuracy: 50.12%\n",
      "Epoch 5/100, Train Loss: 1.00017631, Training Accuracy: 45.97%, Validation Accuracy: 50.36%\n",
      "Epoch 6/100, Train Loss: 1.00024765, Training Accuracy: 45.35%, Validation Accuracy: 50.36%\n",
      "Epoch 7/100, Train Loss: 1.00028476, Training Accuracy: 46.01%, Validation Accuracy: 49.80%\n",
      "Epoch 8/100, Train Loss: 1.00021277, Training Accuracy: 45.39%, Validation Accuracy: 49.41%\n",
      "Epoch 9/100, Train Loss: 1.00021745, Training Accuracy: 46.79%, Validation Accuracy: 50.04%\n",
      "Epoch 10/100, Train Loss: 1.00030070, Training Accuracy: 46.60%, Validation Accuracy: 50.28%\n",
      "Epoch 11/100, Train Loss: 1.00023997, Training Accuracy: 46.64%, Validation Accuracy: 50.59%\n",
      "Epoch 12/100, Train Loss: 1.00020944, Training Accuracy: 47.19%, Validation Accuracy: 52.01%\n",
      "Epoch 13/100, Train Loss: 1.00025430, Training Accuracy: 46.01%, Validation Accuracy: 50.12%\n",
      "Epoch 14/100, Train Loss: 1.00024716, Training Accuracy: 46.60%, Validation Accuracy: 50.91%\n",
      "Epoch 15/100, Train Loss: 1.00023904, Training Accuracy: 46.87%, Validation Accuracy: 50.12%\n",
      "Epoch 16/100, Train Loss: 1.00025106, Training Accuracy: 46.48%, Validation Accuracy: 49.25%\n",
      "Epoch 17/100, Train Loss: 1.00020475, Training Accuracy: 45.58%, Validation Accuracy: 51.14%\n",
      "Epoch 18/100, Train Loss: 1.00014709, Training Accuracy: 45.58%, Validation Accuracy: 50.59%\n",
      "Epoch 19/100, Train Loss: 1.00014662, Training Accuracy: 46.33%, Validation Accuracy: 50.51%\n",
      "Epoch 20/100, Train Loss: 1.00017464, Training Accuracy: 46.01%, Validation Accuracy: 51.54%\n",
      "Epoch 21/100, Train Loss: 1.00018183, Training Accuracy: 46.87%, Validation Accuracy: 48.54%\n",
      "Epoch 22/100, Train Loss: 1.00015077, Training Accuracy: 44.61%, Validation Accuracy: 50.67%\n",
      "Epoch 23/100, Train Loss: 1.00011463, Training Accuracy: 47.07%, Validation Accuracy: 48.07%\n",
      "Epoch 24/100, Train Loss: 1.00019924, Training Accuracy: 47.50%, Validation Accuracy: 49.49%\n",
      "Epoch 25/100, Train Loss: 1.00023550, Training Accuracy: 47.65%, Validation Accuracy: 49.57%\n",
      "Epoch 26/100, Train Loss: 1.00020884, Training Accuracy: 46.29%, Validation Accuracy: 50.36%\n",
      "Epoch 27/100, Train Loss: 1.00016615, Training Accuracy: 46.91%, Validation Accuracy: 50.43%\n",
      "Epoch 28/100, Train Loss: 1.00021019, Training Accuracy: 45.93%, Validation Accuracy: 50.51%\n",
      "Epoch 29/100, Train Loss: 1.00021944, Training Accuracy: 46.09%, Validation Accuracy: 47.99%\n",
      "Epoch 30/100, Train Loss: 1.00024657, Training Accuracy: 47.62%, Validation Accuracy: 49.80%\n",
      "Epoch 31/100, Train Loss: 1.00031747, Training Accuracy: 47.42%, Validation Accuracy: 49.33%\n",
      "Epoch 32/100, Train Loss: 1.00026593, Training Accuracy: 47.19%, Validation Accuracy: 50.75%\n",
      "Epoch 33/100, Train Loss: 1.00046748, Training Accuracy: 47.93%, Validation Accuracy: 49.33%\n",
      "Epoch 34/100, Train Loss: 1.00039971, Training Accuracy: 47.62%, Validation Accuracy: 50.36%\n",
      "Epoch 35/100, Train Loss: 1.00030826, Training Accuracy: 47.89%, Validation Accuracy: 48.86%\n",
      "Epoch 36/100, Train Loss: 1.00028930, Training Accuracy: 47.97%, Validation Accuracy: 50.83%\n",
      "Epoch 37/100, Train Loss: 1.00035966, Training Accuracy: 47.38%, Validation Accuracy: 48.86%\n",
      "Epoch 38/100, Train Loss: 1.00023527, Training Accuracy: 46.25%, Validation Accuracy: 51.14%\n",
      "Epoch 39/100, Train Loss: 1.00022981, Training Accuracy: 46.91%, Validation Accuracy: 51.38%\n",
      "Epoch 40/100, Train Loss: 1.00026421, Training Accuracy: 45.74%, Validation Accuracy: 50.20%\n",
      "Epoch 41/100, Train Loss: 1.00014418, Training Accuracy: 46.68%, Validation Accuracy: 49.33%\n",
      "Epoch 42/100, Train Loss: 1.00013092, Training Accuracy: 46.05%, Validation Accuracy: 50.59%\n",
      "Epoch 43/100, Train Loss: 1.00013508, Training Accuracy: 47.81%, Validation Accuracy: 49.09%\n",
      "Epoch 44/100, Train Loss: 1.00015347, Training Accuracy: 47.30%, Validation Accuracy: 51.54%\n",
      "Epoch 45/100, Train Loss: 1.00013694, Training Accuracy: 46.48%, Validation Accuracy: 48.86%\n",
      "Epoch 46/100, Train Loss: 1.00013715, Training Accuracy: 46.95%, Validation Accuracy: 51.54%\n",
      "Epoch 47/100, Train Loss: 1.00018589, Training Accuracy: 46.13%, Validation Accuracy: 48.07%\n",
      "Epoch 48/100, Train Loss: 1.00016838, Training Accuracy: 47.62%, Validation Accuracy: 50.67%\n",
      "Epoch 49/100, Train Loss: 1.00020625, Training Accuracy: 46.91%, Validation Accuracy: 50.99%\n",
      "Epoch 50/100, Train Loss: 1.00021756, Training Accuracy: 47.50%, Validation Accuracy: 49.17%\n",
      "Epoch 51/100, Train Loss: 1.00020498, Training Accuracy: 46.99%, Validation Accuracy: 51.22%\n",
      "Epoch 52/100, Train Loss: 1.00020906, Training Accuracy: 48.51%, Validation Accuracy: 47.36%\n",
      "Epoch 53/100, Train Loss: 1.00031750, Training Accuracy: 47.42%, Validation Accuracy: 52.49%\n",
      "Epoch 54/100, Train Loss: 1.00040468, Training Accuracy: 48.05%, Validation Accuracy: 48.22%\n",
      "Epoch 55/100, Train Loss: 1.00021020, Training Accuracy: 48.01%, Validation Accuracy: 48.30%\n",
      "Epoch 56/100, Train Loss: 1.00013877, Training Accuracy: 47.30%, Validation Accuracy: 48.54%\n",
      "Epoch 57/100, Train Loss: 1.00014861, Training Accuracy: 48.12%, Validation Accuracy: 51.38%\n",
      "Epoch 58/100, Train Loss: 1.00024421, Training Accuracy: 47.50%, Validation Accuracy: 48.78%\n",
      "Epoch 59/100, Train Loss: 1.00019397, Training Accuracy: 45.97%, Validation Accuracy: 51.38%\n",
      "Epoch 60/100, Train Loss: 1.00015807, Training Accuracy: 46.36%, Validation Accuracy: 51.85%\n",
      "Epoch 61/100, Train Loss: 1.00019603, Training Accuracy: 47.07%, Validation Accuracy: 50.51%\n",
      "Epoch 62/100, Train Loss: 1.00020962, Training Accuracy: 45.62%, Validation Accuracy: 49.01%\n",
      "Epoch 63/100, Train Loss: 1.00014500, Training Accuracy: 46.79%, Validation Accuracy: 51.07%\n",
      "Epoch 64/100, Train Loss: 1.00014290, Training Accuracy: 47.85%, Validation Accuracy: 48.93%\n",
      "Epoch 65/100, Train Loss: 1.00013181, Training Accuracy: 46.17%, Validation Accuracy: 49.09%\n",
      "Epoch 66/100, Train Loss: 1.00014891, Training Accuracy: 45.58%, Validation Accuracy: 50.59%\n",
      "Epoch 67/100, Train Loss: 1.00017515, Training Accuracy: 47.50%, Validation Accuracy: 48.38%\n",
      "Epoch 68/100, Train Loss: 1.00019334, Training Accuracy: 45.82%, Validation Accuracy: 49.49%\n",
      "Epoch 69/100, Train Loss: 1.00022677, Training Accuracy: 47.19%, Validation Accuracy: 48.30%\n",
      "Epoch 70/100, Train Loss: 1.00023671, Training Accuracy: 45.86%, Validation Accuracy: 48.46%\n",
      "Epoch 71/100, Train Loss: 1.00022085, Training Accuracy: 48.67%, Validation Accuracy: 51.46%\n",
      "Epoch 72/100, Train Loss: 1.00028572, Training Accuracy: 47.81%, Validation Accuracy: 50.04%\n",
      "Epoch 73/100, Train Loss: 1.00022398, Training Accuracy: 47.42%, Validation Accuracy: 49.09%\n",
      "Epoch 74/100, Train Loss: 1.00013757, Training Accuracy: 48.12%, Validation Accuracy: 51.93%\n",
      "Epoch 75/100, Train Loss: 1.00026966, Training Accuracy: 48.24%, Validation Accuracy: 49.72%\n",
      "Epoch 76/100, Train Loss: 1.00024398, Training Accuracy: 46.99%, Validation Accuracy: 50.43%\n",
      "Epoch 77/100, Train Loss: 1.00017061, Training Accuracy: 47.81%, Validation Accuracy: 49.57%\n",
      "Epoch 78/100, Train Loss: 1.00024736, Training Accuracy: 48.24%, Validation Accuracy: 48.86%\n",
      "Epoch 79/100, Train Loss: 1.00029188, Training Accuracy: 46.40%, Validation Accuracy: 49.96%\n",
      "Epoch 80/100, Train Loss: 1.00033392, Training Accuracy: 47.93%, Validation Accuracy: 50.91%\n",
      "Epoch 81/100, Train Loss: 1.00017105, Training Accuracy: 46.21%, Validation Accuracy: 50.12%\n",
      "Epoch 82/100, Train Loss: 1.00021139, Training Accuracy: 46.01%, Validation Accuracy: 50.67%\n",
      "Epoch 83/100, Train Loss: 1.00012129, Training Accuracy: 47.15%, Validation Accuracy: 51.07%\n",
      "Epoch 84/100, Train Loss: 1.00012571, Training Accuracy: 45.54%, Validation Accuracy: 50.75%\n",
      "Epoch 85/100, Train Loss: 1.00013172, Training Accuracy: 46.56%, Validation Accuracy: 49.41%\n",
      "Epoch 86/100, Train Loss: 1.00012352, Training Accuracy: 48.40%, Validation Accuracy: 49.57%\n",
      "Epoch 87/100, Train Loss: 1.00016117, Training Accuracy: 47.50%, Validation Accuracy: 48.30%\n",
      "Epoch 88/100, Train Loss: 1.00016891, Training Accuracy: 47.89%, Validation Accuracy: 51.38%\n",
      "Epoch 89/100, Train Loss: 1.00015637, Training Accuracy: 47.07%, Validation Accuracy: 48.93%\n",
      "Epoch 90/100, Train Loss: 1.00018767, Training Accuracy: 46.52%, Validation Accuracy: 51.46%\n",
      "Epoch 91/100, Train Loss: 1.00017685, Training Accuracy: 48.12%, Validation Accuracy: 48.78%\n",
      "Epoch 92/100, Train Loss: 1.00015410, Training Accuracy: 45.31%, Validation Accuracy: 48.46%\n",
      "Epoch 93/100, Train Loss: 1.00014465, Training Accuracy: 45.23%, Validation Accuracy: 49.01%\n",
      "Epoch 94/100, Train Loss: 1.00011458, Training Accuracy: 45.90%, Validation Accuracy: 50.67%\n",
      "Epoch 95/100, Train Loss: 1.00010386, Training Accuracy: 47.46%, Validation Accuracy: 50.67%\n",
      "Epoch 96/100, Train Loss: 1.00015535, Training Accuracy: 46.99%, Validation Accuracy: 49.72%\n",
      "Epoch 97/100, Train Loss: 1.00017463, Training Accuracy: 47.62%, Validation Accuracy: 51.46%\n",
      "Epoch 98/100, Train Loss: 1.00022744, Training Accuracy: 45.97%, Validation Accuracy: 49.57%\n",
      "Epoch 99/100, Train Loss: 1.00022680, Training Accuracy: 48.12%, Validation Accuracy: 49.57%\n",
      "Epoch 100/100, Train Loss: 1.00020775, Training Accuracy: 48.01%, Validation Accuracy: 47.75%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_validation_accuracy</td><td>▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅███████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▅▅▅▆▆▆▅▇▇▆▆▇▇█▇▆▇▇▇▇█▇▆▇█▇▇███▆▆▅▇▇▅▆▇█</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▂▂▁▁▁▂▂▂▃▂▂▁▁▁▂▂▃▁▂▂▁▁▂▂▁▁▂▂▁▁▁▁▁▁▂</td></tr><tr><td>validation_accuracy</td><td>▇▄▅▄▆▅▄▆▂▂▅▆▄▅▆▇▄▇▇▆▇▂▂▇▆▃▂▂▅█▄▅▆▆▂▃▂▆▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_validation_accuracy</td><td>52.48619</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>train_accuracy</td><td>48.00625</td></tr><tr><td>train_loss</td><td>1.00021</td></tr><tr><td>validation_accuracy</td><td>47.75059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-terrain-12</strong> at: <a href='https://wandb.ai/nlp_janninemeier/Project1/runs/lxvm3qn4' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project1/runs/lxvm3qn4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240307_222336-lxvm3qn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming ComparativeClassifier is properly defined\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Project1\", config=config)\n",
    "config = wandb.config\n",
    "\n",
    "# Initialize the model with hyperparameters from wandb\n",
    "model = ComparativeClassifier(input_size=300, hidden_size=config.hidden_dim)\n",
    "\n",
    "# Move your model to the device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss criterion with MarginRankingLoss\n",
    "criterion = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "# Define the optimizer with learning rate from wandb\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Initialize variables for tracking the best scores\n",
    "best_val_accuracy = 0.0  # For tracking the best validation accuracy\n",
    "lowest_train_loss = float('inf')  # For tracking the lowest training loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    total_train, correct_train, epoch_loss = 0, 0, 0.0\n",
    "    \n",
    "    for sentence_vectors, labels in train_loader:\n",
    "        sentence_vectors = sentence_vectors.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(sentence_vectors).squeeze()\n",
    "        target = labels.float()\n",
    "        loss = criterion(outputs, torch.zeros_like(outputs), target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * sentence_vectors.size(0)  # Multiply loss by batch size\n",
    "        predictions_correct = ((outputs > 0) & (target > 0)) | ((outputs < 0) & (target < 0))\n",
    "        correct_train += predictions_correct.sum().item()\n",
    "        total_train += target.size(0)\n",
    "    \n",
    "    epoch_train_loss = epoch_loss / total_train\n",
    "    train_accuracy = correct_train / total_train * 100\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val, correct_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sentence_vectors, labels in val_loader:\n",
    "            sentence_vectors = sentence_vectors.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sentence_vectors).squeeze()\n",
    "            target = labels.float()\n",
    "            \n",
    "            predictions_correct = ((outputs > 0) & (target > 0)) | ((outputs < 0) & (target < 0))\n",
    "            correct_val += predictions_correct.sum().item()\n",
    "            total_val += target.size(0)\n",
    "            \n",
    "    validation_accuracy = correct_val / total_val * 100\n",
    "\n",
    "    # Update best scores if current scores are better and optionally save the best model\n",
    "    if validation_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = validation_accuracy\n",
    "        torch.save(model.state_dict(), 'model_best_val_accuracy.pth')\n",
    "   \n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": epoch_train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"validation_accuracy\": validation_accuracy,\n",
    "        \"best_validation_accuracy\": best_val_accuracy,\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{config.num_epochs}, '\n",
    "          f'Train Loss: {epoch_train_loss:.8f}, '\n",
    "          f'Training Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f97a73-e2fd-4c95-9743-b82be7bc325d",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Once I have the best model,  I run it on the test set to get the final performance metrics (here just for demonstration as it is again run on validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7d9cd20-839a-42ba-a4ff-0cbd54d5d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.49%\n"
     ]
    }
   ],
   "source": [
    "# Path to your saved model\n",
    "path_to_best_model = 'model_best_val_accuracy.pth'\n",
    "\n",
    "# Load the best model state\n",
    "model.load_state_dict(torch.load(path_to_best_model, map_location=device))\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track test performance\n",
    "total_test, correct_test = 0, 0\n",
    "\n",
    "# No gradient updates needed during evaluation\n",
    "with torch.no_grad():\n",
    "    for sentence_vectors, labels in test_loader:\n",
    "        sentence_vectors = sentence_vectors.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass to get outputs\n",
    "        outputs = model(sentence_vectors).squeeze()\n",
    "        \n",
    "        # Convert labels to a float tensor\n",
    "        target = labels.float()\n",
    "        \n",
    "        # Determine prediction correctness\n",
    "        predictions_correct = ((outputs > 0) & (target > 0)) | ((outputs < 0) & (target < 0))\n",
    "        \n",
    "        # Update correct predictions and total count\n",
    "        correct_test += predictions_correct.sum().item()\n",
    "        total_test += target.size(0)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = correct_test / total_test * 100\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1017f82-430c-460e-8860-7031c0251c99",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "Achieving a test accuracy of 52.49 in a task where the probability of guessing correctly is 50% indicates that my model is performing slightly better than random chance. However, the improvement over random guessing is minimal. Here's my interpretation of these results and the steps I might consider taking next:\n",
    "\n",
    "### My Interpretation\n",
    "- **Marginally Better Than Random**: This performance suggests that my model has learned some patterns from the data relevant to the task at hand, but not significantly so. The marginal improvement could stem from various fact for example thek's complex - considering even GPT-4 only got 87.5% accuracy - see https://paperswithcode.com/sota/common-sense-reasoning-on-winogrande. Other factors might bety, the representativeness of my training dta, or the model architecture and hyperparameters I chose.\n",
    "- **Overfitting Concerns**: If my training accuracy was much higher than my validation accuracy (e.g. run 128/1e-4), it might indicate that my model overfitted the training data. This means it learned the training data's specific patterns so well that it failed to generalize to new, unseen data.\n",
    "- **Underfitting Concerns**: Conversely, if both training and validation accuracies are low - which they are with <53%, my model might be underfitting. This could happen if the model is too simplistic to capture the underlying data patterns or hasn't been trained sufficiently.\n",
    "\n",
    "### My Next Steps\n",
    "- **Enhance DataQuantity**: I should  also have a \"real\" test set.\n",
    "- **Revisit Feature Engineering**: Given that I'm using averaged word vectors, I should consider whether there's a more effective way to represent my text data. For some tasks, different types of embeddings or more complex text representations might better capture the nuances.\n",
    "- **Adjust Model Complexity**: I need to evaluate whether my model's architecture suits the task. Increasing the model's complexity might help, but it also raises the risk of overfitting. Simplifying the model can sometimes improve its ability to generalize.\n",
    "- **Continue Hyperparameter Tuning**: Further tuning might be necessary. The small margin by which my model exceeds random guessing suggests there might still be optimization room.\n",
    "- **Incorporate Regularization and Dropout**: If overfitting is an issue, adding regularization techniques or dropout layers could help my model avoid memorizing the training data.\n",
    "- **Implement Cross-validation**: Using cross-validation could provide a more robust performance estimate and help prevent overfitting by ensuring that the model performs well across different data subsets.\n",
    "\n",
    "In summary, while my model shows some capacity to learn from the training data, the performance on the test set, close to random, indicates there's significant room for improvement. I'll need to carefully consider these factors and experiment with different approaches to enhance my model's generalization cawith thatlities and thus improve its (real) performance  unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c715e4c-9a19-42f5-a358-7914d4c10134",
   "metadata": {},
   "source": [
    "### References\n",
    "I used ChatGPT and the uploaded files on ILIAS to assist me with coding in this project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
