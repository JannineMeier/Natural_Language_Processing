{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hEKSaMMrBps"
      },
      "source": [
        "# Project 5: LLM\n",
        "\n",
        "This is the fifth project in NLP SW013 done by Jannine Meier.\n",
        "\n",
        "The project was run on Google Colab.\n",
        "\n",
        "WandB Project Link with created View options: https://wandb.ai/jannine-meier/project5_JM?nw=nwuserjanninemeier\n",
        "\n",
        "### Project description\n",
        "This project involves further fine-tuning a pretrained LLM from huggingface on the Winogrande dataset. I chose the quantized Llama-3-8b-bnb-4bit model from Unsloth for my project: https://huggingface.co/unsloth/llama-3-8b-bnb-4bit\n",
        "\n",
        "## Prefix\n",
        "\n",
        "I wanted to give a quick explanation to this notebook. I spent at least 45 hours trying to complete it, even took 2 days off work, but still faced so many challenges that I was not able to complete the project as requested. I used different base models and tried many tutorials to solve the task, started over and over again but kept running into memory issues as described in my email. New appraoches same problem. My colleagues tried to help but couldn’t solve the problem either. Discussing with other students I did not get any useful information as the five people I spoke to seemed all to struggle themselves.\n",
        "\n",
        "After trying on GPUHub for many days I tried to run it on google colab by Wednesday night, where it worked better (at least no longer had memory issues) but it was too late to finish everything as I still had other issues to solve now that the code finally ran and every now and then I got the message that GPU is no longer available. This means that every 2 hours I was not able to use the GPU without paying for it...\n",
        "\n",
        "Below in this notebook, Ive written down what I planned to do, but I wasn’t able to finish it and execute it fully. Overall, I found the task very challenging. For me personally, and maybe also other students without an apprenticeship in IT and being fast coders, it was overwhelming to teach myself the necessary skills and correctly combine them so that the code would run, all within such a short time frame.\n",
        "\n",
        "In short what I wanted to express is that I actually wasn't a sloth (just like my model) and that neither the lack of trying nor investing time was the reason for the notebook being as it is and I hope this will be considered when grading. I'm sure with more time or more gpu power or some more guidance (like with the last projects during the lessons) I would have been able to solve it in a better way. Nevertheless, I definetely learned a looot during this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRZe8k9I057i"
      },
      "source": [
        "## Libraries & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installation of necessary packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rgJRkUmAvmTv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Importing necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from torch.nn.functional import cross_entropy\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import wandb\n",
        "\n",
        "# Initializing Weights & Biases for experiment tracking\n",
        "wandb.init(project=\"project5_JM\", entity=\"jannine-meier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLrwrzVR1BhE"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "**Quantization:** I use a 4-bit quantization to reduce memory usage.\n",
        "\n",
        "**Load Datasets:** I chose winogrande_l as I already had not enough time but taking a bigger one would for sure be beneficial for the training. I load and split the datasets into training, validation, and test sets. For the Winogrande dataset, I specifically extracted the last 1000 entries from the training dataset to create the test set.\n",
        "\n",
        "**Tokenizer Usage:** I use the tokenizer for the \"unsloth/llama-3-8b-bnb-4bit\" model which uses a Byte-Pair Encoding approach to handle subwords. Tokens 128000 and 128001 are reserved for start and end of text tokens. I also set the padding token to 128001.\n",
        "- **Not Removing Punctuation and Stopwords:** I decided to retain punctuation and stopwords because they sometimes have significant contextual meanings which might be crucial for the model to understand nuanced differences between sentences.\n",
        "- **No Stemming or Lemmatization:** I decided that these processes are unnecessary, as my tokenizer uses a subword tokenization method capable of understanding various word forms without reducing them to their root forms.\n",
        "\n",
        "\n",
        "**Prompt Formatting for Model Input:**\n",
        "- Alpaca_prompt: I used a template string that structures the input to the model. It formats an instruction, a contextual input, and expects a response in a structured format. I used the same base alpaca-prompt and instruction for all the data. For the input I structured the sentence + option1 + option2 and for the ouput I pasted the raw label as 1 or 2 reffering to the correct option1 and option2 followed by a EOS token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1yVsLQv3JU6",
        "outputId": "8f870bc0-0e18-43f4-daed-365710327d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation and manipulation\n",
        "winogrande_datasets = load_dataset('winogrande', 'winogrande_l')\n",
        "train_dataset = winogrande_datasets['train'].select(range(len(winogrande_datasets['train']) - 1000))\n",
        "eval_dataset = winogrande_datasets['validation']\n",
        "test_dataset = winogrande_datasets['train'].select(range(len(winogrande_datasets['train']) - 1000, len(winogrande_datasets['train'])))\n",
        "\n",
        "# Load pre-trained model and tokenizer with specific configurations\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 128,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Set the padding side to left (did not work but could not figure out why)\n",
        "# tokenizer.padding_side = 'left'\n",
        "\n",
        "# Set up end-of-sequence token using tokenizer's built-in attribute\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Enhance the model with PEFT modifications\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Finetune on 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,  # Finetune on 8, 16, 32, 64, 128\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized according to unsloth documentation\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized according to unsloth documentation\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # Supports rank stabilized LoRA (could be a possible additional finetune parameter)\n",
        "    loftq_config = None, # Supports LoftQ (could be a possible attional finetune parameter)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjY75GoYUCB8",
        "outputId": "18a2c2d7-a38a-4a21-9ab4-f16f48461979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that contains only one digit either 1 or 2 and no other words or symbols.\n",
            "\n",
            "### Instruction:\n",
            "Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "\n",
            "### Input:\n",
            "Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine. Option 1: Ian Option 2: Dennis\n",
            "\n",
            "### Response:\n",
            "2<|end_of_text|>\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that contains only one digit either 1 or 2 and no other words or symbols.\n",
            "\n",
            "### Instruction:\n",
            "Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "\n",
            "### Input:\n",
            "Ian volunteered to eat Dennis's menudo after already having a bowl because _ enjoyed eating intestine. Option 1: Ian Option 2: Dennis\n",
            "\n",
            "### Response:\n",
            "1<|end_of_text|>\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that contains only one digit either 1 or 2 and no other words or symbols.\n",
            "\n",
            "### Instruction:\n",
            "Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "\n",
            "### Input:\n",
            "He never comes to my home, but I always go to his house because the _ is smaller. Option 1: home Option 2: house\n",
            "\n",
            "### Response:\n",
            "1<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Define a prompt template for constructing dataset entries\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that contains only one digit either 1 or 2 and no other words or symbols.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\" + EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples, include_answers=False):\n",
        "    # Generate instructions for each example\n",
        "    instructions = [\"Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\"] * len(examples['sentence'])\n",
        "    # Combine sentences with their options to create the input text\n",
        "    inputs = [f\"{examples['sentence'][i]} Option 1: {examples['option1'][i]} Option 2: {examples['option2'][i]}\" for i in range(len(examples['sentence']))]\n",
        "    # Only include the correct answer in the output for train_dataset\n",
        "    outputs = [examples['answer'][i] if include_answers else '' for i in range(len(examples['sentence']))]\n",
        "    # Format each example into the full prompt text\n",
        "    texts = [alpaca_prompt.format(instr, inp, out) for instr, inp, out in zip(instructions, inputs, outputs)]\n",
        "    return {'text': texts}\n",
        "\n",
        "# Apply the formatting function to the datasets\n",
        "train_dataset = train_dataset.map(lambda examples: formatting_prompts_func(examples, include_answers=True), batched=True)\n",
        "# Excludes answers for evaluation and testing datasets\n",
        "eval_dataset = eval_dataset.map(lambda examples: formatting_prompts_func(examples, include_answers=False), batched=True)\n",
        "test_dataset = test_dataset.map(lambda examples: formatting_prompts_func(examples, include_answers=False), batched=True)\n",
        "\n",
        "# Print the first 5 formatted examples from the training dataset to check output\n",
        "for i in range(3):\n",
        "    print(train_dataset[i]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "## Model\n",
        "\n",
        "I chose the Llama-3-8b-bnb-4bit model as it is trained on 15 trillion tokens which is a looot.\n",
        "\n",
        "\n",
        "Due to time constraints I was not able to test out many LoRA adapter configurations but in the comments in the code you can see what I would have liked to test out with sufficient time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73udq5dl6CGn"
      },
      "source": [
        "## Training\n",
        "I used the Huggingface's SFTTrainer. This setup is specifically tailored for efficient and effective fine-tuning of large language models, making use of advanced training techniques like low precision computing and parameter-efficient model adaptations.\n",
        "\n",
        "Goal would be to run at least one or more epochs but I had not even close enough time to do this so i just trained on a few examples... I set the max_steps to 500 to see how the loss performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 128,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences according to Unsloth\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs=1, goal\n",
        "        # max_steps = 0, goal\n",
        "        max_steps = 10, # chose this due to time constraints\n",
        "        learning_rate = 2e-4, # should be fine-tuned\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"wandb\",  # This will enable logging to Weights & Biases\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "b3cccb8f-3ec7-42b2-d648-9d69b855b3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.594 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "70fffb52-ba01-4b68-9038-4b901af5d93e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:37, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.917900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.999600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.946400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.705900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.386700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.061200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.889900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_mAD0IgTI1r"
      },
      "source": [
        "Current status: My code is finally running and I have 6 hours left and it takes me 7-8 minutes to train 100 samples. This means to train the whole train samples which are over 10k it would take me over 11 hours which I do not have left. I'll stick with fewer samples due to time constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "5aa9fa9d-e5fb-4df8-8753-0290f851eb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47.3741 seconds used for training.\n",
            "0.79 minutes used for training.\n",
            "Peak reserved memory = 6.172 GB.\n",
            "Peak reserved memory for training = 0.578 GB.\n",
            "Peak reserved memory % of max memory = 41.85 %.\n",
            "Peak reserved memory for training % of max memory = 3.919 %.\n"
          ]
        }
      ],
      "source": [
        "# Shows final memory and time stats to check how long it takes and if memory is sufficient\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xS9sV9vjOV6C"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, tokenizer, dataset):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0  # Initialize total_predictions\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for index, example in enumerate(dataset):\n",
        "        if index >= 10:  # Stop after processing 30 entries due to time constraints\n",
        "            break\n",
        "\n",
        "        prompt = example['text']\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding='longest').to(\"cuda\")\n",
        "\n",
        "        # Generate logits\n",
        "        outputs = model(**inputs, max_new_tokens=128, return_dict=True)\n",
        "        logits = outputs.logits  # Logits for all tokens\n",
        "        last_token_logits = logits[:, -1, :]  # Focus on the last token for classification\n",
        "\n",
        "        # Adjust labels for cross_entropy\n",
        "        label = int(example['answer']) - 1  # Convert labels from '1' and '2' to '0' and '1'\n",
        "        labels = torch.tensor([label], dtype=torch.long).to(\"cuda\")\n",
        "\n",
        "        # Calculate loss for the last token\n",
        "        loss = cross_entropy(last_token_logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Decode full generated response\n",
        "        full_generated_tokens = model.generate(**inputs, max_length=tokenizer.model_max_length)\n",
        "        full_response_text = tokenizer.decode(full_generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "        predicted_number = full_response_text.strip()[-1]  # Extract the last character as the prediction\n",
        "\n",
        "        # Check correctness\n",
        "        correct = (predicted_number == example['answer'])\n",
        "        correct_predictions += correct\n",
        "\n",
        "        # Detailed output for each example\n",
        "        instruction = prompt.split(\"### Instruction:\")[1].split(\"### Input:\")[0].strip()\n",
        "        input_text = prompt.split(\"### Input:\")[1].split(\"### Response:\")[0].strip()\n",
        "        response_text = prompt.split(\"### Response:\")[1].strip().split(tokenizer.eos_token)[0]\n",
        "        actual_answer = example['answer']\n",
        "\n",
        "        print(f\"Example {index + 1}:\")\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Input: {input_text}\")\n",
        "        print(f\"Predicted Response: {predicted_number}\")\n",
        "        print(f\"Actual Response: {actual_answer}\")\n",
        "        print(f\"---\")\n",
        "\n",
        "        total_predictions += 1  # Update total_predictions within the loop\n",
        "\n",
        "    # Calculate final metrics\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    average_loss = total_loss / total_predictions\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    wandb.log({\"Validation Accuracy\": accuracy, \"Validation Loss\": average_loss})\n",
        "\n",
        "    return accuracy, average_loss\n",
        "\n",
        "# Assuming your dataset is already loaded and formatted correctly\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "test_dataset = test_dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foZa1dc2KMD1",
        "outputId": "89815d4e-2e54-4572-db64-5bb0fe0755c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: Sarah was a much better surgeon than Maria so _ always got the easier cases. Option 1: Sarah Option 2: Maria\n",
            "Predicted Response: 2\n",
            "Actual Response: 2\n",
            "---\n",
            "Example 2:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: Sarah was a much better surgeon than Maria so _ always got the harder cases. Option 1: Sarah Option 2: Maria\n",
            "Predicted Response: y\n",
            "Actual Response: 1\n",
            "---\n",
            "Example 3:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: They were worried the wine would ruin the bed and the blanket, but the _ was't ruined. Option 1: blanket Option 2: bed\n",
            "Predicted Response: 1\n",
            "Actual Response: 2\n",
            "---\n",
            "Example 4:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: Terry tried to bake the eggplant in the toaster oven but the _ was too big. Option 1: eggplant Option 2: toaster\n",
            "Predicted Response: 1\n",
            "Actual Response: 1\n",
            "---\n",
            "Example 5:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: At night, Jeffrey always stays up later than Hunter to watch TV because _ wakes up late. Option 1: Jeffrey Option 2: Hunter\n",
            "Predicted Response: /\n",
            "Actual Response: 1\n",
            "---\n",
            "Example 6:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: The cat of Sarah has some mouth problems, so she takes it to see Maria. _ is a responsible cat owner. Option 1: Sarah Option 2: Maria\n",
            "Predicted Response: 1\n",
            "Actual Response: 1\n",
            "---\n",
            "Example 7:\n",
            "Instruction: Choose which option is the right one to replace the underscore and make the sentence meaningful and answer with a number (1 or 2) only\n",
            "Input: The home that my parents had when I was in school was a lot nicer than my house now because the _ was sophisticated. Option 1: home Option 2: house\n",
            "Predicted Response: 2\n",
            "Actual Response: 1\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on a subset of the validation dataset\n",
        "accuracy, average_loss = validate_model(model, tokenizer, eval_dataset)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}, Validation Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl67uGXdOioh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Saving, loading finetuned models\n",
        "This ONLY saves the LoRA adapters, and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmp5J45QOiYm"
      },
      "source": [
        "## Results\n",
        "\n",
        "\n",
        "\n",
        "In the last projects my model was not showing any real signs of learning as not even the train accuracy improved over time. Validation accuracy project4: 0.553, project3: 0.509.\n",
        "\n",
        "This time I see when I look at the training loss it looks like the model is learning quiet well. Even though I had no time to run a full epoch on the train set I noticed that the trend of decreasing loss is persisting which is a good sign and shows that probably training a full epoch would result in even better results. It would be beneficial to also plot a learning accuracy curve but I did not have the time to specialize the trainer to do the logging.\n",
        "\n",
        "On the validation set my results were different depending on how many samples I used for learning as well as how many for validating (obviously). As I was not able to run the full sets my results are not representative and fluctuating a lot depending on the chosen sizes. I therefore also left out the confusion matrix which would not be saying a lot.\n",
        "\n",
        "Actually I noticed the more training samples the worse the model performs.. This means that probably my setup has some issues which need to be fixed and that the basemodel without my finetuning performs better than with it :')...\n",
        "\n",
        "E.g.: train & validation 100 samples\n",
        "- Validation accuracy: 0.14 (predics a lot of \"/\" instead of numbers)\n",
        "\n",
        "E.g.: train 30 samples & validation 50 samples\n",
        "- Validation accuracy: 0.48\n",
        "\n",
        "\n",
        "So I evaluated the test accuracy after training only with 30 sample (almost not at all) and got following accuracy which probably will be the best accuracy overall.. but as I said acutally doesn't make sense to look at these numbers\n",
        ".\n",
        "- Test accuracy: 0.53 (project4: 0.662, project3: 0.500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4f5OeIJKNHV"
      },
      "outputs": [],
      "source": [
        "# Evaluate on a subset of the test dataset\n",
        "accuracy, average_loss = validate_model(model, tokenizer, test_dataset)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}, Test Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arFBkVRrOfAl"
      },
      "source": [
        "## Interpretation\n",
        "\n",
        "Due to time constraints I was not able to actually test out any finetuning parameters as commented in the code and also some prompt engineering would most likely help to get better results. This means my results are not really representing bunch of experiments but just looking at the numbers I can say its not (yet) working as it should which could be due to many facotrs like not enough training data, not enough finetuning, not right checking for correct output format, not perfect prompts, etc.\n",
        "\n",
        "Looking at the predicted ouput it seems that the model sometimes predicts the right answer but often also not a number (only) instead symbols like / or . or ) and quiet often also the wrong answer resp. label. When looking at the full generated answers from the model I notice that it sometimes answers with the word instead of the number label and sometimes in full or part sentences instead of just the numbers. This means I either have to finetune it better or improve my instruction and alpaca prompt so the model actually learns the exact pattern i expect or I should change the way I check my models ouputs correctnes maybe to check for a number in the whole generated response answer ouput after the response tag and not only the last token or maybe also look for the words resp. the options in letters instead of the number.\n",
        "\n",
        "I am certain that with the right instructions and finetuning as well as enough training (e.g. training with a large training set) we actually should be able to get good scores for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "My notebook is inspired from Unsloth where they have tutorials on fine-tuning Llama models: https://github.com/unslothai/unsloth?tab=readme-ov-file\n",
        "\n",
        "Additionally I used snippets from huggingface especially: https://huggingface.co/docs/trl/sft_trainer\n",
        "\n",
        "Also used code from the notebook shown here: https://www.youtube.com/watch?v=pK8u4QfdLx0\n",
        "\n",
        "ChatGPT was used to help me clean the code and make adjustments where needed."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
