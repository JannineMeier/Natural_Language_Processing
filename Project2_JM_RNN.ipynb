{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: RNNs\n",
    "\n",
    "This is the second project in NLP SW05 done by Jannine Meier. \n",
    "\n",
    "The project was run on GPUHub.\n",
    "\n",
    "WandB: \n",
    "- https://wandb.ai/nlp_janninemeier/Project2_Winogrande\n",
    "- https://wandb.ai/nlp_janninemeier/Project2_Anagram\n",
    "- https://wandb.ai/nlp_janninemeier/Project2_Palindrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.11/site-packages (0.16.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (1.43.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install numpy pandas torch datasets transformers wandb matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)  # Python\n",
    "    np.random.seed(seed_value)  # NumPy\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch, if you are using CUDA\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloading\n",
    "The datasets are loaded and split into training, validation, and test sets.\n",
    "\n",
    "Winogrande is split by excluding the last 1000 entries from the full training dataset and form the test set with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'NLP/SW05'\n",
      "/home/jovyan/NLP/SW05\n"
     ]
    }
   ],
   "source": [
    "# Load Winogrande dataset\n",
    "def load_winogrande_dataset():\n",
    "    winogrande_dataset = load_dataset('winogrande', 'winogrande_l')\n",
    "    wg_train_dataset = winogrande_dataset['train'].select(range(len(winogrande_dataset['train']) - 1000))\n",
    "    wg_val_dataset = winogrande_dataset['validation']\n",
    "    wg_test_dataset = winogrande_dataset['train'].select(range(len(winogrande_dataset['train']) - 1000, len(winogrande_dataset['train'])))\n",
    "    return wg_train_dataset, wg_val_dataset, wg_test_dataset\n",
    "\n",
    "# Go to the direcotry where the anagram and palindrome datasets are saved (needs to be adjusted if code has to be reproduced)\n",
    "%pwd\n",
    "%cd 'NLP/SW05'\n",
    "%pwd\n",
    "\n",
    "# Load anagram dataset\n",
    "def load_anagram_dataset(train_path, valid_path, test_path):\n",
    "    anagram_train = pd.read_csv(train_path, header=None, names=['sequence', 'label'])\n",
    "    anagram_valid = pd.read_csv(valid_path, header=None, names=['sequence', 'label'])\n",
    "    anagram_test = pd.read_csv(test_path, header=None, names=['sequence', 'label'])\n",
    "    return anagram_train, anagram_valid, anagram_test\n",
    "\n",
    "# Load palindrome dataset\n",
    "def load_palindrome_dataset(train_path, valid_path, test_path):\n",
    "    palindrome_train = pd.read_csv(train_path, header=None, names=['text', 'label'])\n",
    "    palindrome_valid = pd.read_csv(valid_path, header=None, names=['text', 'label'])\n",
    "    palindrome_test = pd.read_csv(test_path, header=None, names=['text', 'label'])\n",
    "    return palindrome_train, palindrome_valid, palindrome_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose dataset for the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  text  label\n",
      "0    j q f m m s f q j      0\n",
      "1      v y f u u f y v      1\n",
      "2          a c c k k a      0\n",
      "3  v y d m e w m t y v      0\n",
      "4            l s j j l      0\n",
      "Length train set: 1000\n",
      "Length val set: 500\n",
      "Length test set: 500\n"
     ]
    }
   ],
   "source": [
    "# Choose which dataset to load and print some entries\n",
    "dataset_choice = 'palindrome'  # Change this variable to switch between datasets\n",
    "\n",
    "if dataset_choice == 'winogrande':\n",
    "    wg_train_dataset, wg_val_dataset, wg_test_dataset = load_winogrande_dataset()\n",
    "    print(wg_train_dataset[:1])\n",
    "    print(\"Length train set:\", len(wg_train_dataset))\n",
    "    print(\"Length val set:\",len(wg_val_dataset))\n",
    "    print(\"Length test set:\",len(wg_test_dataset))\n",
    "elif dataset_choice == 'anagram':\n",
    "    anagram_train, anagram_valid, anagram_test = load_anagram_dataset('anagram_train.csv', 'anagram_valid.csv', 'anagram_test.csv')\n",
    "    print(anagram_train.head())\n",
    "    print(\"Length train set:\", len(anagram_train))\n",
    "    print(\"Length val set:\", len(anagram_valid))\n",
    "    print(\"Length test set:\", len(anagram_test))\n",
    "\n",
    "elif dataset_choice == 'palindrome':\n",
    "    palindrome_train, palindrome_valid, palindrome_test = load_palindrome_dataset('palindrome_train.csv', 'palindrome_valid.csv', 'palindrome_test.csv')\n",
    "    print(palindrome_train.head())\n",
    "    print(\"Length train set:\",len(palindrome_train))\n",
    "    print(\"Length val set:\",len(palindrome_valid))\n",
    "    print(\"Length test set:\",len(palindrome_test))\n",
    "else:\n",
    "    print(\"Invalid dataset choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "**Winogrande dataset**\n",
    "- I start by replacing a placeholder in the sentences same as I did in the last project.\n",
    "- Next, I tokenize these sentences with the BertTokenizer. With the tokenizer I convert the text to tokens (lowercase words). Then I apply padding and truncation to ensure uniform length and use a maximum length of 128 to keep the data manageable. I chose the maximum length because the mean of the sentence sizes is around 90, so we do not waste too much computation on padding tokens even though the acutal max size of the datasets is 186.\n",
    "- The conversion of labels from '1' and '2' to 0 and 1 is a done because it is a binary classification task and it aligns with the  format used later in my code.\n",
    "- Concatenating the input_ids to form a single input sequence for each example. To avoid that my model interprets them as two separate sequences, I remove the first token of the second sequence (the duplicated [CLS] token) before concatenation. I tested and ensured that the concatenated result does not exceed the model's maximum sequence length. \n",
    "\n",
    "**Anagram and Palindrome dataset**\n",
    "- I've chosen to implement a custom CharTokenizer, which makes sense because character-level features are important for these tasks. It maps each character to a unique integer ID and sets a special entry for padding, mapped to the ID 0 which is used to pad sequences to uniform lengths during tokenization (the padding itself is done later in the custom_collate_fn function).\n",
    "- For anagram I split the sequence into two parts at the separator and tokenize each part separately by removing spaces. The two results, as well as the separator which I mapped to ID 1 are then combined. As discussed shortly during class it is important to not remove the seperator in order to preserve the structure for analyizing if it is an anagram. \n",
    "- For palindrome I tokenize the entire sequence as one single unit by removing spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed palindrome training sample label: 0\n",
      "Preprocessed palindrome training sample sequence (not yet padded): [7, 8, 4, 9, 9, 4, 8, 7]\n",
      "Vocabulary size: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7760/2802681824.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sequence, label = row[0], int(row[1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess Winogrande dataset\n",
    "def preprocess_winogrande_dataset(dataset, tokenizer):\n",
    "    def preprocess_function(examples):\n",
    "        # Replace the blank with each option and create two sequences\n",
    "        first_sentences = [sentence.replace('_', option) for sentence, option in zip(examples['sentence'], examples['option1'])]\n",
    "        second_sentences = [sentence.replace('_', option) for sentence, option in zip(examples['sentence'], examples['option2'])]\n",
    "        # Tokenize both sets of sequences\n",
    "        first_tokenized = tokenizer(first_sentences, padding='max_length', truncation=True, max_length=128)\n",
    "        second_tokenized = tokenizer(second_sentences, padding='max_length', truncation=True, max_length=128)\n",
    "        # Convert labels '1' and '2' in examples['answer'] to 0 and 1\n",
    "        converted_labels = [int(label) - 1 for label in examples['answer']]\n",
    "        # Prepare the final features \n",
    "        features = {\n",
    "            'input_ids': [first_tokenized['input_ids'][i] + second_tokenized['input_ids'][i][1:] for i in range(len(examples['sentence']))],  # Remove first token of the second sequence to avoid duplicating CLS\n",
    "            'labels': converted_labels\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    return dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        # Initialize with a padding token\n",
    "        self.char_to_id = {\"[PAD]\": 0, \"<sep>\": 1}  # Add separator token\n",
    "        self.id_to_char = {0: \"[PAD]\", 1: \"<sep>\"}  # Reverse mapping\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokenized_text = []\n",
    "        for char in text:\n",
    "            if char not in self.char_to_id:\n",
    "                char_id = len(self.char_to_id)\n",
    "                self.char_to_id[char] = char_id\n",
    "                self.id_to_char[char_id] = char\n",
    "            tokenized_text.append(self.char_to_id[char])\n",
    "        return tokenized_text\n",
    "\n",
    "# Preprocess anagram or palindrome dataset\n",
    "def preprocess_dataset(dataframe, tokenizer, is_palindrome=False):\n",
    "    processed_data = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sequence, label = row[0], int(row[1])\n",
    "        if is_palindrome:\n",
    "            tokenized_sequence = tokenizer.tokenize(sequence.replace(' ', ''))\n",
    "            processed_data.append((tokenized_sequence, label))\n",
    "        else:\n",
    "            part1, part2 = sequence.split('<sep>')\n",
    "            tokenized_part1 = tokenizer.tokenize(part1.replace(' ', ''))\n",
    "            separator_token = [1]\n",
    "            tokenized_part2 = tokenizer.tokenize(part2.replace(' ', ''))\n",
    "            processed_data.append((tokenized_part1 + separator_token + tokenized_part2, label))\n",
    "            \n",
    "    return processed_data\n",
    "\n",
    "    \n",
    "# Choose which dataset to preprocess\n",
    "if dataset_choice == 'winogrande':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    wg_train_dataset = preprocess_winogrande_dataset(wg_train_dataset, tokenizer)\n",
    "    wg_val_dataset = preprocess_winogrande_dataset(wg_val_dataset, tokenizer)\n",
    "    wg_test_dataset = preprocess_winogrande_dataset(wg_test_dataset, tokenizer)\n",
    "    # Show a sample\n",
    "    print(\"Preprocessed winogrande training sample input_ids (padded):\", wg_train_dataset['input_ids'][0])\n",
    "    print(\"Preprocessed winogrande training sample labels:\", wg_train_dataset['labels'][0])\n",
    "    print(\"Vocabulary size:\", len(tokenizer.vocab))\n",
    "\n",
    "elif dataset_choice == 'anagram':\n",
    "    tokenizer = CharTokenizer() \n",
    "    anagram_train_features = preprocess_dataset(anagram_train, tokenizer, is_palindrome=False)\n",
    "    anagram_valid_features = preprocess_dataset(anagram_valid, tokenizer, is_palindrome=False)\n",
    "    anagram_test_features = preprocess_dataset(anagram_test, tokenizer, is_palindrome=False)\n",
    "    # Show a sample\n",
    "    print(\"Preprocessed anagram training sample sequence (not yet padded, seperated by ID 1):\", anagram_train_features[0][0])\n",
    "    print(\"Preprocessed anagram training sample label:\", anagram_train_features[0][1])\n",
    "    print(\"Vocabulary size:\", len(tokenizer.char_to_id))\n",
    "    \n",
    "elif dataset_choice == 'palindrome':\n",
    "    tokenizer = CharTokenizer()\n",
    "    palindrome_train_features = preprocess_dataset(palindrome_train, tokenizer, is_palindrome=True)\n",
    "    palindrome_valid_features = preprocess_dataset(palindrome_valid, tokenizer, is_palindrome=True)\n",
    "    palindrome_test_features = preprocess_dataset(palindrome_test, tokenizer, is_palindrome=True)\n",
    "    # Show a sample\n",
    "    print(\"Preprocessed palindrome training sample label:\", palindrome_train_features[0][1])\n",
    "    print(\"Preprocessed palindrome training sample sequence (not yet padded):\", palindrome_train_features[1][0])\n",
    "    print(\"Vocabulary size:\", len(tokenizer.char_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations\n",
    "\n",
    "I tried out different settings for my runs to get high accuracies ranging from:\n",
    "- I used a learning_rate of 1e-2 to 1e-5 as these are common values for hyperparametertuning.\n",
    "- I set epochs up to 100 for winogrande and up to 500 for anagram/palindrome. I limited this because of the time consumption in regards to this project. Another reason for the limiting was that I got overfitting already after early epochs so I came to conlusion to go with 50 epochs for Winogrande and 250 for the other two datasets. \n",
    "- I set the train_batch_size as big as possible without running into memory error (of course still considering that very large batch sizes can lead to poorer generalization but I was not able to go over 64 anyway because of memory).\n",
    "- I worked with different embedding_dim and hidden_dim ranging from 64 to 51.\n",
    "- I used dropout between 0.1 to 0.3 to prevent overfitting. As I saw a lot of overfitting I used the 0.3 for my final runs. \n",
    "- The input_dim is either the vocabulary size from the CharTokenizer dictionary or the one from BertTokenizer so it fits the dataset which is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hz0nsz7d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_valid_acc</td><td>▁████</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▁▇█▂</td></tr><tr><td>train_loss</td><td>█▃▁▂▁</td></tr><tr><td>valid_acc</td><td>▁██▁█</td></tr><tr><td>valid_loss</td><td>█▁▃▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_valid_acc</td><td>0.50434</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>test_accuracy</td><td>0.5</td></tr><tr><td>train_acc</td><td>0.49459</td></tr><tr><td>train_loss</td><td>0.69334</td></tr><tr><td>valid_acc</td><td>0.50434</td></tr><tr><td>valid_loss</td><td>0.69315</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-terrain-11</strong> at: <a href='https://wandb.ai/nlp_janninemeier/Project2_Winogrande/runs/hz0nsz7d' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project2_Winogrande/runs/hz0nsz7d</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240321_224159-hz0nsz7d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hz0nsz7d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NLP/SW05/wandb/run-20240321_225035-r8hr7uyv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_janninemeier/Project2_Palindrome/runs/r8hr7uyv' target=\"_blank\">dulcet-universe-11</a></strong> to <a href='https://wandb.ai/nlp_janninemeier/Project2_Palindrome' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_janninemeier/Project2_Palindrome' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project2_Palindrome</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_janninemeier/Project2_Palindrome/runs/r8hr7uyv' target=\"_blank\">https://wandb.ai/nlp_janninemeier/Project2_Palindrome/runs/r8hr7uyv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Weights & Biases configuration of hyperparameters\n",
    "default_config = {\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"epochs\": 250,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 64,\n",
    "    \"test_batch_size\": 64,\n",
    "    \"embedding_dim\": 64, \n",
    "    \"hidden_dim\":64, \n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.3, \n",
    "    \"input_dim\": None # Is updatet depending on the used dataset/tokenizer\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Project2_Palindrome\", config=default_config)\n",
    "config = wandb.config\n",
    "\n",
    "# Update input_dim based on tokenizer's vocabulary\n",
    "if hasattr(tokenizer, 'vocab'):\n",
    "    config.update({\"input_dim\": len(tokenizer.vocab)}, allow_val_change=True)\n",
    "\n",
    "else:\n",
    "    config.update({\"input_dim\": len(tokenizer.char_to_id)}, allow_val_change=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Here I instantiate a recurrent neural network classifier which is designed for binary classification tasks and chose an LSTM layer as its core component for handling sequences. As you suggested, I chose nn.Embedding to convert letters, words etc. into vectors.\n",
    "\n",
    "As most of my inputs are padded, the last hidden layer entries are mostly all zeros which do not really represent the data. Therefore I ignore the padding by taking the packed embeddings and the actual lengths of each sequence (text_lengths) to produce a packed sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)  # batch_size x seq_len x embedding_dim\n",
    "\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # We only need the last hidden state\n",
    "        hidden = hidden[-1].squeeze(0)  # Final hidden state of the last layer\n",
    "\n",
    "        output = self.classifier(hidden)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Instantiate the model\n",
    "model = RNNClassifier(\n",
    "    input_dim=config.input_dim,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    "    output_dim=1,  # Binary classification\n",
    "    num_layers=config.num_layers,\n",
    "    dropout=config.dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "I chose the BCEWithLogitsLoss with an already integrated sigmoid layer because it's specifically designed for binary classification tasks.\n",
    "\n",
    "I chose the Adam optimizer which is a common choice for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss function\n",
    "loss_function = nn.BCEWithLogitsLoss() \n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# We can check if a GPU is available and move our model there\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader\n",
    "\n",
    "**Custom Collate Function**:\n",
    "For BertTokenizer data in dictionary format, it assembles input_ids into tensors, stacks them, and creates a tensor for the labels (it was already padded during preprocessing so the pad_sequence does not make a difference here).\n",
    "For CharTokenizer data in tuple format, it pads sequences to ensure uniform length to the longest sequence in the batch. I intentionally do not take the mean length of sequences to pad (as I did for the Winogrande dataset) because here I want to always have the full sequence as otherwise the model has not really a chance to learn successfully.\n",
    "\n",
    "**DataLoader**:\n",
    "I shuffle (only) the train sets to ensure randomness, which can help improve my model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoaders for the training, validation, and test sets\n",
    "train_batch_size = config.train_batch_size\n",
    "eval_batch_size = config.eval_batch_size\n",
    "test_batch_size = config.test_batch_size\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # For both BERT-like tokenized data and CharTokenizer data\n",
    "    input_ids, labels = zip(*[(item['input_ids'] if 'input_ids' in item else item[0], item['labels'] if 'labels' in item else item[1]) for item in batch])\n",
    "    input_ids = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in input_ids], batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    # Calculate sequence lengths by counting non-zero entries (non-padding tokens)\n",
    "    sequence_lengths = torch.tensor([seq.gt(0).sum() for seq in input_ids], dtype=torch.long)\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels, 'sequence_lengths': sequence_lengths}\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # data is expected to be a list of tuples (sequence, label)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.data[idx]\n",
    "        # Convert sequence and label into PyTorch tensors\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "        return sequence_tensor, label_tensor\n",
    "\n",
    "# Choose which dataset to create DataLoader for\n",
    "if dataset_choice == 'winogrande':\n",
    "    train_loader = DataLoader(wg_train_dataset, train_batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(wg_val_dataset, eval_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(wg_test_dataset, test_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "elif dataset_choice == 'anagram':\n",
    "    train_loader = DataLoader(CustomDataset(anagram_train_features), train_batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(CustomDataset(anagram_valid_features), eval_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(CustomDataset(anagram_test_features), test_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "elif dataset_choice == 'palindrome':\n",
    "    train_loader = DataLoader(CustomDataset(palindrome_train_features), train_batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(CustomDataset(palindrome_valid_features), eval_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(CustomDataset(palindrome_test_features), test_batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validate loop\n",
    "\n",
    "Here I pass input data through the train and validation model to get predictions and compute the loss between predictions and actual labels.\n",
    "\n",
    "Before the train model goes through a backward pass, I zeroe the gradients to prevent accumulation from previous iterations.\n",
    "\n",
    "Finally, both train and validation return the average loss and accuracy for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        sequence_lengths = batch['sequence_lengths'].to(device)\n",
    "        predictions = model(input_ids, sequence_lengths).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Convert logits to probabilities for binary classification\n",
    "        predicted_probs = torch.sigmoid(predictions)\n",
    "        predicted = (predicted_probs >= 0.5).long()  # Threshold predictions at 0.5\n",
    "        total += labels.size(0)\n",
    "        epoch_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / total\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            sequence_lengths = batch['sequence_lengths'].to(device)\n",
    "            predictions = model(input_ids, sequence_lengths).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            predicted_probs = torch.sigmoid(predictions)\n",
    "            predicted = (predicted_probs >= 0.5).long()  \n",
    "            total += labels.size(0)\n",
    "            epoch_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The loop iterates over the specified number of epochs.\n",
    "\n",
    "If the validation loss is lower than the previously recorded best validation loss (best_valid_loss), the model's weights are deep-copied and stored (best_model_wts).\n",
    "\n",
    "After all epochs, the best model weights are saved to a file named 'best_model.pt' and also saved on wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7760/2804166533.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in input_ids], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "Train Loss: 0.697 | Train Acc: 49.70%\n",
      "Val Loss: 0.695 | Val Acc: 50.60000000000000142109%\n",
      "Epoch 2/250\n",
      "Train Loss: 0.686 | Train Acc: 57.20%\n",
      "Val Loss: 0.692 | Val Acc: 52.60000000000000142109%\n",
      "Epoch 3/250\n",
      "Train Loss: 0.664 | Train Acc: 60.90%\n",
      "Val Loss: 0.688 | Val Acc: 50.60000000000000142109%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/250\n",
      "Train Loss: 0.621 | Train Acc: 67.80%\n",
      "Val Loss: 0.727 | Val Acc: 51.60000000000000142109%\n",
      "Epoch 5/250\n",
      "Train Loss: 0.548 | Train Acc: 72.90%\n",
      "Val Loss: 0.785 | Val Acc: 52.80000000000000426326%\n",
      "Epoch 6/250\n",
      "Train Loss: 0.496 | Train Acc: 77.60%\n",
      "Val Loss: 0.870 | Val Acc: 51.80000000000000426326%\n",
      "Epoch 7/250\n",
      "Train Loss: 0.407 | Train Acc: 82.00%\n",
      "Val Loss: 0.965 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 8/250\n",
      "Train Loss: 0.379 | Train Acc: 83.00%\n",
      "Val Loss: 1.058 | Val Acc: 52.60000000000000142109%\n",
      "Epoch 9/250\n",
      "Train Loss: 0.315 | Train Acc: 86.70%\n",
      "Val Loss: 1.150 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 10/250\n",
      "Train Loss: 0.271 | Train Acc: 88.90%\n",
      "Val Loss: 1.162 | Val Acc: 53.60000000000000142109%\n",
      "Epoch 11/250\n",
      "Train Loss: 0.237 | Train Acc: 90.70%\n",
      "Val Loss: 1.324 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 12/250\n",
      "Train Loss: 0.201 | Train Acc: 92.10%\n",
      "Val Loss: 1.227 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 13/250\n",
      "Train Loss: 0.173 | Train Acc: 92.90%\n",
      "Val Loss: 1.452 | Val Acc: 53.60000000000000142109%\n",
      "Epoch 14/250\n",
      "Train Loss: 0.143 | Train Acc: 94.20%\n",
      "Val Loss: 1.673 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 15/250\n",
      "Train Loss: 0.167 | Train Acc: 92.60%\n",
      "Val Loss: 1.428 | Val Acc: 59.19999999999999573674%\n",
      "Epoch 16/250\n",
      "Train Loss: 0.160 | Train Acc: 94.60%\n",
      "Val Loss: 1.599 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 17/250\n",
      "Train Loss: 0.147 | Train Acc: 94.60%\n",
      "Val Loss: 1.638 | Val Acc: 54.80000000000000426326%\n",
      "Epoch 18/250\n",
      "Train Loss: 0.190 | Train Acc: 92.90%\n",
      "Val Loss: 1.466 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 19/250\n",
      "Train Loss: 0.147 | Train Acc: 95.20%\n",
      "Val Loss: 1.488 | Val Acc: 52.80000000000000426326%\n",
      "Epoch 20/250\n",
      "Train Loss: 0.111 | Train Acc: 95.90%\n",
      "Val Loss: 1.690 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 21/250\n",
      "Train Loss: 0.098 | Train Acc: 96.50%\n",
      "Val Loss: 1.891 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 22/250\n",
      "Train Loss: 0.100 | Train Acc: 96.00%\n",
      "Val Loss: 1.816 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 23/250\n",
      "Train Loss: 0.116 | Train Acc: 95.00%\n",
      "Val Loss: 1.749 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 24/250\n",
      "Train Loss: 0.116 | Train Acc: 95.40%\n",
      "Val Loss: 1.934 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 25/250\n",
      "Train Loss: 0.146 | Train Acc: 94.80%\n",
      "Val Loss: 1.776 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 26/250\n",
      "Train Loss: 0.168 | Train Acc: 93.30%\n",
      "Val Loss: 1.520 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 27/250\n",
      "Train Loss: 0.140 | Train Acc: 95.30%\n",
      "Val Loss: 1.634 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 28/250\n",
      "Train Loss: 0.121 | Train Acc: 95.00%\n",
      "Val Loss: 1.729 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 29/250\n",
      "Train Loss: 0.107 | Train Acc: 95.30%\n",
      "Val Loss: 1.794 | Val Acc: 53.80000000000000426326%\n",
      "Epoch 30/250\n",
      "Train Loss: 0.092 | Train Acc: 96.60%\n",
      "Val Loss: 1.818 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 31/250\n",
      "Train Loss: 0.083 | Train Acc: 96.90%\n",
      "Val Loss: 1.896 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 32/250\n",
      "Train Loss: 0.076 | Train Acc: 97.10%\n",
      "Val Loss: 1.845 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 33/250\n",
      "Train Loss: 0.096 | Train Acc: 96.00%\n",
      "Val Loss: 1.841 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 34/250\n",
      "Train Loss: 0.091 | Train Acc: 96.70%\n",
      "Val Loss: 1.768 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 35/250\n",
      "Train Loss: 0.080 | Train Acc: 96.80%\n",
      "Val Loss: 1.844 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 36/250\n",
      "Train Loss: 0.095 | Train Acc: 96.20%\n",
      "Val Loss: 1.826 | Val Acc: 59.39999999999999857891%\n",
      "Epoch 37/250\n",
      "Train Loss: 0.092 | Train Acc: 97.00%\n",
      "Val Loss: 1.955 | Val Acc: 53.80000000000000426326%\n",
      "Epoch 38/250\n",
      "Train Loss: 0.107 | Train Acc: 96.50%\n",
      "Val Loss: 1.770 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 39/250\n",
      "Train Loss: 0.087 | Train Acc: 97.10%\n",
      "Val Loss: 1.833 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 40/250\n",
      "Train Loss: 0.081 | Train Acc: 97.20%\n",
      "Val Loss: 1.790 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 41/250\n",
      "Train Loss: 0.061 | Train Acc: 97.70%\n",
      "Val Loss: 1.873 | Val Acc: 54.80000000000000426326%\n",
      "Epoch 42/250\n",
      "Train Loss: 0.059 | Train Acc: 97.70%\n",
      "Val Loss: 2.005 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 43/250\n",
      "Train Loss: 0.085 | Train Acc: 96.80%\n",
      "Val Loss: 2.111 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 44/250\n",
      "Train Loss: 0.107 | Train Acc: 96.60%\n",
      "Val Loss: 2.002 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 45/250\n",
      "Train Loss: 0.091 | Train Acc: 96.60%\n",
      "Val Loss: 1.889 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 46/250\n",
      "Train Loss: 0.067 | Train Acc: 97.60%\n",
      "Val Loss: 2.155 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 47/250\n",
      "Train Loss: 0.071 | Train Acc: 97.30%\n",
      "Val Loss: 2.202 | Val Acc: 51.20000000000000284217%\n",
      "Epoch 48/250\n",
      "Train Loss: 0.061 | Train Acc: 97.30%\n",
      "Val Loss: 2.117 | Val Acc: 53.20000000000000284217%\n",
      "Epoch 49/250\n",
      "Train Loss: 0.088 | Train Acc: 96.40%\n",
      "Val Loss: 2.185 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 50/250\n",
      "Train Loss: 0.087 | Train Acc: 96.50%\n",
      "Val Loss: 2.046 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 51/250\n",
      "Train Loss: 0.088 | Train Acc: 96.50%\n",
      "Val Loss: 1.971 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 52/250\n",
      "Train Loss: 0.085 | Train Acc: 96.80%\n",
      "Val Loss: 1.916 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 53/250\n",
      "Train Loss: 0.067 | Train Acc: 97.40%\n",
      "Val Loss: 2.001 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 54/250\n",
      "Train Loss: 0.055 | Train Acc: 98.20%\n",
      "Val Loss: 2.109 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 55/250\n",
      "Train Loss: 0.036 | Train Acc: 98.90%\n",
      "Val Loss: 2.250 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 56/250\n",
      "Train Loss: 0.066 | Train Acc: 97.50%\n",
      "Val Loss: 2.212 | Val Acc: 53.80000000000000426326%\n",
      "Epoch 57/250\n",
      "Train Loss: 0.095 | Train Acc: 96.80%\n",
      "Val Loss: 2.192 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 58/250\n",
      "Train Loss: 0.073 | Train Acc: 97.70%\n",
      "Val Loss: 2.026 | Val Acc: 57.99999999999999289457%\n",
      "Epoch 59/250\n",
      "Train Loss: 0.071 | Train Acc: 97.80%\n",
      "Val Loss: 2.048 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 60/250\n",
      "Train Loss: 0.051 | Train Acc: 98.10%\n",
      "Val Loss: 2.141 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 61/250\n",
      "Train Loss: 0.050 | Train Acc: 97.90%\n",
      "Val Loss: 2.281 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 62/250\n",
      "Train Loss: 0.058 | Train Acc: 97.90%\n",
      "Val Loss: 2.336 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 63/250\n",
      "Train Loss: 0.064 | Train Acc: 97.50%\n",
      "Val Loss: 2.077 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 64/250\n",
      "Train Loss: 0.078 | Train Acc: 97.90%\n",
      "Val Loss: 2.052 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 65/250\n",
      "Train Loss: 0.066 | Train Acc: 97.80%\n",
      "Val Loss: 2.094 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 66/250\n",
      "Train Loss: 0.064 | Train Acc: 97.60%\n",
      "Val Loss: 2.052 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 67/250\n",
      "Train Loss: 0.077 | Train Acc: 96.90%\n",
      "Val Loss: 2.119 | Val Acc: 53.60000000000000142109%\n",
      "Epoch 68/250\n",
      "Train Loss: 0.067 | Train Acc: 97.70%\n",
      "Val Loss: 2.171 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 69/250\n",
      "Train Loss: 0.078 | Train Acc: 97.10%\n",
      "Val Loss: 2.186 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 70/250\n",
      "Train Loss: 0.058 | Train Acc: 98.70%\n",
      "Val Loss: 2.271 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 71/250\n",
      "Train Loss: 0.086 | Train Acc: 97.30%\n",
      "Val Loss: 2.308 | Val Acc: 52.80000000000000426326%\n",
      "Epoch 72/250\n",
      "Train Loss: 0.124 | Train Acc: 95.40%\n",
      "Val Loss: 2.007 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 73/250\n",
      "Train Loss: 0.147 | Train Acc: 95.30%\n",
      "Val Loss: 1.897 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 74/250\n",
      "Train Loss: 0.119 | Train Acc: 95.60%\n",
      "Val Loss: 1.829 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 75/250\n",
      "Train Loss: 0.132 | Train Acc: 95.40%\n",
      "Val Loss: 1.820 | Val Acc: 53.40000000000000568434%\n",
      "Epoch 76/250\n",
      "Train Loss: 0.114 | Train Acc: 95.70%\n",
      "Val Loss: 1.917 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 77/250\n",
      "Train Loss: 0.100 | Train Acc: 95.60%\n",
      "Val Loss: 2.038 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 78/250\n",
      "Train Loss: 0.107 | Train Acc: 96.40%\n",
      "Val Loss: 2.065 | Val Acc: 53.40000000000000568434%\n",
      "Epoch 79/250\n",
      "Train Loss: 0.092 | Train Acc: 96.50%\n",
      "Val Loss: 1.986 | Val Acc: 52.80000000000000426326%\n",
      "Epoch 80/250\n",
      "Train Loss: 0.090 | Train Acc: 96.80%\n",
      "Val Loss: 2.074 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 81/250\n",
      "Train Loss: 0.086 | Train Acc: 96.40%\n",
      "Val Loss: 2.117 | Val Acc: 52.00000000000000000000%\n",
      "Epoch 82/250\n",
      "Train Loss: 0.070 | Train Acc: 97.10%\n",
      "Val Loss: 2.190 | Val Acc: 53.00000000000000000000%\n",
      "Epoch 83/250\n",
      "Train Loss: 0.069 | Train Acc: 97.40%\n",
      "Val Loss: 2.165 | Val Acc: 52.60000000000000142109%\n",
      "Epoch 84/250\n",
      "Train Loss: 0.056 | Train Acc: 97.50%\n",
      "Val Loss: 2.100 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 85/250\n",
      "Train Loss: 0.075 | Train Acc: 97.30%\n",
      "Val Loss: 2.130 | Val Acc: 52.40000000000000568434%\n",
      "Epoch 86/250\n",
      "Train Loss: 0.064 | Train Acc: 98.20%\n",
      "Val Loss: 2.278 | Val Acc: 53.60000000000000142109%\n",
      "Epoch 87/250\n",
      "Train Loss: 0.083 | Train Acc: 97.50%\n",
      "Val Loss: 2.020 | Val Acc: 53.00000000000000000000%\n",
      "Epoch 88/250\n",
      "Train Loss: 0.095 | Train Acc: 96.30%\n",
      "Val Loss: 2.066 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 89/250\n",
      "Train Loss: 0.091 | Train Acc: 96.20%\n",
      "Val Loss: 2.049 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 90/250\n",
      "Train Loss: 0.075 | Train Acc: 97.40%\n",
      "Val Loss: 1.845 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 91/250\n",
      "Train Loss: 0.094 | Train Acc: 96.50%\n",
      "Val Loss: 2.084 | Val Acc: 54.80000000000000426326%\n",
      "Epoch 92/250\n",
      "Train Loss: 0.075 | Train Acc: 97.90%\n",
      "Val Loss: 2.053 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 93/250\n",
      "Train Loss: 0.107 | Train Acc: 96.70%\n",
      "Val Loss: 2.005 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 94/250\n",
      "Train Loss: 0.099 | Train Acc: 96.40%\n",
      "Val Loss: 2.152 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 95/250\n",
      "Train Loss: 0.103 | Train Acc: 96.00%\n",
      "Val Loss: 2.081 | Val Acc: 57.19999999999999573674%\n",
      "Epoch 96/250\n",
      "Train Loss: 0.102 | Train Acc: 96.10%\n",
      "Val Loss: 2.218 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 97/250\n",
      "Train Loss: 0.090 | Train Acc: 96.50%\n",
      "Val Loss: 2.086 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 98/250\n",
      "Train Loss: 0.097 | Train Acc: 96.50%\n",
      "Val Loss: 2.188 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 99/250\n",
      "Train Loss: 0.093 | Train Acc: 96.40%\n",
      "Val Loss: 2.121 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 100/250\n",
      "Train Loss: 0.098 | Train Acc: 96.60%\n",
      "Val Loss: 2.224 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 101/250\n",
      "Train Loss: 0.104 | Train Acc: 96.30%\n",
      "Val Loss: 2.110 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 102/250\n",
      "Train Loss: 0.097 | Train Acc: 95.80%\n",
      "Val Loss: 1.977 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 103/250\n",
      "Train Loss: 0.105 | Train Acc: 95.90%\n",
      "Val Loss: 2.153 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 104/250\n",
      "Train Loss: 0.116 | Train Acc: 95.70%\n",
      "Val Loss: 2.151 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 105/250\n",
      "Train Loss: 0.143 | Train Acc: 94.70%\n",
      "Val Loss: 1.943 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 106/250\n",
      "Train Loss: 0.117 | Train Acc: 95.80%\n",
      "Val Loss: 1.903 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 107/250\n",
      "Train Loss: 0.179 | Train Acc: 93.90%\n",
      "Val Loss: 1.820 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 108/250\n",
      "Train Loss: 0.141 | Train Acc: 95.20%\n",
      "Val Loss: 1.781 | Val Acc: 58.59999999999999431566%\n",
      "Epoch 109/250\n",
      "Train Loss: 0.151 | Train Acc: 94.40%\n",
      "Val Loss: 1.691 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 110/250\n",
      "Train Loss: 0.127 | Train Acc: 95.20%\n",
      "Val Loss: 1.789 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 111/250\n",
      "Train Loss: 0.092 | Train Acc: 96.20%\n",
      "Val Loss: 1.823 | Val Acc: 54.80000000000000426326%\n",
      "Epoch 112/250\n",
      "Train Loss: 0.111 | Train Acc: 95.70%\n",
      "Val Loss: 1.781 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 113/250\n",
      "Train Loss: 0.114 | Train Acc: 95.50%\n",
      "Val Loss: 1.871 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 114/250\n",
      "Train Loss: 0.105 | Train Acc: 95.70%\n",
      "Val Loss: 1.957 | Val Acc: 57.99999999999999289457%\n",
      "Epoch 115/250\n",
      "Train Loss: 0.129 | Train Acc: 95.40%\n",
      "Val Loss: 1.840 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 116/250\n",
      "Train Loss: 0.095 | Train Acc: 96.00%\n",
      "Val Loss: 1.854 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 117/250\n",
      "Train Loss: 0.084 | Train Acc: 96.80%\n",
      "Val Loss: 1.796 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 118/250\n",
      "Train Loss: 0.087 | Train Acc: 96.90%\n",
      "Val Loss: 1.861 | Val Acc: 58.59999999999999431566%\n",
      "Epoch 119/250\n",
      "Train Loss: 0.104 | Train Acc: 96.10%\n",
      "Val Loss: 1.935 | Val Acc: 56.59999999999999431566%\n",
      "Epoch 120/250\n",
      "Train Loss: 0.075 | Train Acc: 97.20%\n",
      "Val Loss: 1.907 | Val Acc: 59.19999999999999573674%\n",
      "Epoch 121/250\n",
      "Train Loss: 0.088 | Train Acc: 96.90%\n",
      "Val Loss: 1.957 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 122/250\n",
      "Train Loss: 0.069 | Train Acc: 97.50%\n",
      "Val Loss: 1.988 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 123/250\n",
      "Train Loss: 0.074 | Train Acc: 97.20%\n",
      "Val Loss: 1.917 | Val Acc: 61.00000000000000000000%\n",
      "Epoch 124/250\n",
      "Train Loss: 0.084 | Train Acc: 96.10%\n",
      "Val Loss: 2.045 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 125/250\n",
      "Train Loss: 0.083 | Train Acc: 97.10%\n",
      "Val Loss: 2.027 | Val Acc: 57.99999999999999289457%\n",
      "Epoch 126/250\n",
      "Train Loss: 0.079 | Train Acc: 97.00%\n",
      "Val Loss: 2.029 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 127/250\n",
      "Train Loss: 0.083 | Train Acc: 96.90%\n",
      "Val Loss: 2.120 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 128/250\n",
      "Train Loss: 0.108 | Train Acc: 96.70%\n",
      "Val Loss: 2.093 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 129/250\n",
      "Train Loss: 0.105 | Train Acc: 97.20%\n",
      "Val Loss: 2.141 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 130/250\n",
      "Train Loss: 0.102 | Train Acc: 96.40%\n",
      "Val Loss: 1.877 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 131/250\n",
      "Train Loss: 0.139 | Train Acc: 94.50%\n",
      "Val Loss: 1.889 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 132/250\n",
      "Train Loss: 0.136 | Train Acc: 94.80%\n",
      "Val Loss: 1.826 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 133/250\n",
      "Train Loss: 0.161 | Train Acc: 93.90%\n",
      "Val Loss: 1.679 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 134/250\n",
      "Train Loss: 0.174 | Train Acc: 93.40%\n",
      "Val Loss: 1.790 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 135/250\n",
      "Train Loss: 0.154 | Train Acc: 94.80%\n",
      "Val Loss: 1.682 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 136/250\n",
      "Train Loss: 0.147 | Train Acc: 93.40%\n",
      "Val Loss: 1.679 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 137/250\n",
      "Train Loss: 0.133 | Train Acc: 94.40%\n",
      "Val Loss: 1.763 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 138/250\n",
      "Train Loss: 0.106 | Train Acc: 95.80%\n",
      "Val Loss: 1.705 | Val Acc: 58.79999999999999715783%\n",
      "Epoch 139/250\n",
      "Train Loss: 0.124 | Train Acc: 95.20%\n",
      "Val Loss: 1.818 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 140/250\n",
      "Train Loss: 0.116 | Train Acc: 96.10%\n",
      "Val Loss: 1.874 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 141/250\n",
      "Train Loss: 0.115 | Train Acc: 95.70%\n",
      "Val Loss: 1.876 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 142/250\n",
      "Train Loss: 0.133 | Train Acc: 95.00%\n",
      "Val Loss: 1.793 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 143/250\n",
      "Train Loss: 0.135 | Train Acc: 95.10%\n",
      "Val Loss: 1.822 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 144/250\n",
      "Train Loss: 0.123 | Train Acc: 95.30%\n",
      "Val Loss: 1.841 | Val Acc: 57.99999999999999289457%\n",
      "Epoch 145/250\n",
      "Train Loss: 0.106 | Train Acc: 97.00%\n",
      "Val Loss: 1.803 | Val Acc: 58.19999999999999573674%\n",
      "Epoch 146/250\n",
      "Train Loss: 0.092 | Train Acc: 95.80%\n",
      "Val Loss: 1.857 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 147/250\n",
      "Train Loss: 0.107 | Train Acc: 95.40%\n",
      "Val Loss: 1.909 | Val Acc: 58.19999999999999573674%\n",
      "Epoch 148/250\n",
      "Train Loss: 0.078 | Train Acc: 97.30%\n",
      "Val Loss: 1.800 | Val Acc: 59.59999999999999431566%\n",
      "Epoch 149/250\n",
      "Train Loss: 0.087 | Train Acc: 96.40%\n",
      "Val Loss: 1.934 | Val Acc: 58.79999999999999715783%\n",
      "Epoch 150/250\n",
      "Train Loss: 0.096 | Train Acc: 96.70%\n",
      "Val Loss: 1.755 | Val Acc: 59.19999999999999573674%\n",
      "Epoch 151/250\n",
      "Train Loss: 0.108 | Train Acc: 95.70%\n",
      "Val Loss: 1.857 | Val Acc: 59.39999999999999857891%\n",
      "Epoch 152/250\n",
      "Train Loss: 0.086 | Train Acc: 97.10%\n",
      "Val Loss: 1.888 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 153/250\n",
      "Train Loss: 0.097 | Train Acc: 96.20%\n",
      "Val Loss: 1.832 | Val Acc: 59.19999999999999573674%\n",
      "Epoch 154/250\n",
      "Train Loss: 0.105 | Train Acc: 96.40%\n",
      "Val Loss: 1.867 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 155/250\n",
      "Train Loss: 0.082 | Train Acc: 97.00%\n",
      "Val Loss: 1.882 | Val Acc: 58.59999999999999431566%\n",
      "Epoch 156/250\n",
      "Train Loss: 0.102 | Train Acc: 96.00%\n",
      "Val Loss: 1.993 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 157/250\n",
      "Train Loss: 0.081 | Train Acc: 97.20%\n",
      "Val Loss: 2.164 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 158/250\n",
      "Train Loss: 0.070 | Train Acc: 97.40%\n",
      "Val Loss: 2.097 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 159/250\n",
      "Train Loss: 0.105 | Train Acc: 96.00%\n",
      "Val Loss: 1.985 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 160/250\n",
      "Train Loss: 0.116 | Train Acc: 96.30%\n",
      "Val Loss: 2.103 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 161/250\n",
      "Train Loss: 0.110 | Train Acc: 96.00%\n",
      "Val Loss: 1.881 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 162/250\n",
      "Train Loss: 0.112 | Train Acc: 95.80%\n",
      "Val Loss: 1.957 | Val Acc: 58.19999999999999573674%\n",
      "Epoch 163/250\n",
      "Train Loss: 0.145 | Train Acc: 94.90%\n",
      "Val Loss: 1.994 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 164/250\n",
      "Train Loss: 0.109 | Train Acc: 95.90%\n",
      "Val Loss: 1.945 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 165/250\n",
      "Train Loss: 0.103 | Train Acc: 96.70%\n",
      "Val Loss: 1.872 | Val Acc: 56.59999999999999431566%\n",
      "Epoch 166/250\n",
      "Train Loss: 0.108 | Train Acc: 96.00%\n",
      "Val Loss: 1.853 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 167/250\n",
      "Train Loss: 0.164 | Train Acc: 94.80%\n",
      "Val Loss: 1.869 | Val Acc: 59.00000000000000000000%\n",
      "Epoch 168/250\n",
      "Train Loss: 0.132 | Train Acc: 94.90%\n",
      "Val Loss: 1.787 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 169/250\n",
      "Train Loss: 0.133 | Train Acc: 94.90%\n",
      "Val Loss: 1.860 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 170/250\n",
      "Train Loss: 0.145 | Train Acc: 94.20%\n",
      "Val Loss: 1.783 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 171/250\n",
      "Train Loss: 0.110 | Train Acc: 95.40%\n",
      "Val Loss: 1.773 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 172/250\n",
      "Train Loss: 0.113 | Train Acc: 95.50%\n",
      "Val Loss: 1.825 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 173/250\n",
      "Train Loss: 0.107 | Train Acc: 95.60%\n",
      "Val Loss: 1.916 | Val Acc: 56.59999999999999431566%\n",
      "Epoch 174/250\n",
      "Train Loss: 0.116 | Train Acc: 94.90%\n",
      "Val Loss: 1.897 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 175/250\n",
      "Train Loss: 0.142 | Train Acc: 95.10%\n",
      "Val Loss: 1.860 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 176/250\n",
      "Train Loss: 0.106 | Train Acc: 95.80%\n",
      "Val Loss: 1.829 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 177/250\n",
      "Train Loss: 0.105 | Train Acc: 95.90%\n",
      "Val Loss: 1.865 | Val Acc: 59.19999999999999573674%\n",
      "Epoch 178/250\n",
      "Train Loss: 0.130 | Train Acc: 95.90%\n",
      "Val Loss: 1.886 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 179/250\n",
      "Train Loss: 0.104 | Train Acc: 95.30%\n",
      "Val Loss: 1.860 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 180/250\n",
      "Train Loss: 0.113 | Train Acc: 95.10%\n",
      "Val Loss: 1.836 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 181/250\n",
      "Train Loss: 0.103 | Train Acc: 96.30%\n",
      "Val Loss: 1.811 | Val Acc: 58.79999999999999715783%\n",
      "Epoch 182/250\n",
      "Train Loss: 0.091 | Train Acc: 96.40%\n",
      "Val Loss: 1.792 | Val Acc: 58.59999999999999431566%\n",
      "Epoch 183/250\n",
      "Train Loss: 0.100 | Train Acc: 96.20%\n",
      "Val Loss: 1.822 | Val Acc: 59.59999999999999431566%\n",
      "Epoch 184/250\n",
      "Train Loss: 0.091 | Train Acc: 96.20%\n",
      "Val Loss: 1.864 | Val Acc: 59.39999999999999857891%\n",
      "Epoch 185/250\n",
      "Train Loss: 0.102 | Train Acc: 96.80%\n",
      "Val Loss: 1.909 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 186/250\n",
      "Train Loss: 0.100 | Train Acc: 95.60%\n",
      "Val Loss: 1.900 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 187/250\n",
      "Train Loss: 0.091 | Train Acc: 96.80%\n",
      "Val Loss: 1.829 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 188/250\n",
      "Train Loss: 0.082 | Train Acc: 97.30%\n",
      "Val Loss: 1.915 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 189/250\n",
      "Train Loss: 0.085 | Train Acc: 97.10%\n",
      "Val Loss: 1.878 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 190/250\n",
      "Train Loss: 0.082 | Train Acc: 96.80%\n",
      "Val Loss: 1.929 | Val Acc: 58.39999999999999857891%\n",
      "Epoch 191/250\n",
      "Train Loss: 0.094 | Train Acc: 96.90%\n",
      "Val Loss: 1.972 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 192/250\n",
      "Train Loss: 0.076 | Train Acc: 97.40%\n",
      "Val Loss: 2.111 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 193/250\n",
      "Train Loss: 0.077 | Train Acc: 97.00%\n",
      "Val Loss: 2.032 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 194/250\n",
      "Train Loss: 0.085 | Train Acc: 96.70%\n",
      "Val Loss: 1.978 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 195/250\n",
      "Train Loss: 0.074 | Train Acc: 96.90%\n",
      "Val Loss: 1.974 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 196/250\n",
      "Train Loss: 0.061 | Train Acc: 97.80%\n",
      "Val Loss: 2.089 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 197/250\n",
      "Train Loss: 0.071 | Train Acc: 97.50%\n",
      "Val Loss: 2.175 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 198/250\n",
      "Train Loss: 0.076 | Train Acc: 97.50%\n",
      "Val Loss: 2.057 | Val Acc: 57.19999999999999573674%\n",
      "Epoch 199/250\n",
      "Train Loss: 0.084 | Train Acc: 97.00%\n",
      "Val Loss: 2.199 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 200/250\n",
      "Train Loss: 0.125 | Train Acc: 95.40%\n",
      "Val Loss: 2.046 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 201/250\n",
      "Train Loss: 0.089 | Train Acc: 96.80%\n",
      "Val Loss: 2.019 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 202/250\n",
      "Train Loss: 0.117 | Train Acc: 95.60%\n",
      "Val Loss: 2.078 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 203/250\n",
      "Train Loss: 0.128 | Train Acc: 96.00%\n",
      "Val Loss: 2.078 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 204/250\n",
      "Train Loss: 0.100 | Train Acc: 96.20%\n",
      "Val Loss: 2.141 | Val Acc: 52.40000000000000568434%\n",
      "Epoch 205/250\n",
      "Train Loss: 0.090 | Train Acc: 97.10%\n",
      "Val Loss: 2.029 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 206/250\n",
      "Train Loss: 0.091 | Train Acc: 96.70%\n",
      "Val Loss: 2.083 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 207/250\n",
      "Train Loss: 0.084 | Train Acc: 97.00%\n",
      "Val Loss: 2.013 | Val Acc: 55.00000000000000710543%\n",
      "Epoch 208/250\n",
      "Train Loss: 0.076 | Train Acc: 96.80%\n",
      "Val Loss: 2.054 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 209/250\n",
      "Train Loss: 0.069 | Train Acc: 97.90%\n",
      "Val Loss: 2.272 | Val Acc: 53.80000000000000426326%\n",
      "Epoch 210/250\n",
      "Train Loss: 0.109 | Train Acc: 95.50%\n",
      "Val Loss: 2.029 | Val Acc: 54.80000000000000426326%\n",
      "Epoch 211/250\n",
      "Train Loss: 0.099 | Train Acc: 96.30%\n",
      "Val Loss: 2.004 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 212/250\n",
      "Train Loss: 0.130 | Train Acc: 94.80%\n",
      "Val Loss: 2.044 | Val Acc: 53.00000000000000000000%\n",
      "Epoch 213/250\n",
      "Train Loss: 0.127 | Train Acc: 95.60%\n",
      "Val Loss: 1.958 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 214/250\n",
      "Train Loss: 0.090 | Train Acc: 96.80%\n",
      "Val Loss: 1.833 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 215/250\n",
      "Train Loss: 0.123 | Train Acc: 95.10%\n",
      "Val Loss: 1.893 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 216/250\n",
      "Train Loss: 0.098 | Train Acc: 96.30%\n",
      "Val Loss: 1.916 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 217/250\n",
      "Train Loss: 0.098 | Train Acc: 96.00%\n",
      "Val Loss: 1.903 | Val Acc: 57.99999999999999289457%\n",
      "Epoch 218/250\n",
      "Train Loss: 0.071 | Train Acc: 97.90%\n",
      "Val Loss: 1.890 | Val Acc: 57.39999999999999857891%\n",
      "Epoch 219/250\n",
      "Train Loss: 0.067 | Train Acc: 97.80%\n",
      "Val Loss: 1.988 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 220/250\n",
      "Train Loss: 0.057 | Train Acc: 98.10%\n",
      "Val Loss: 2.016 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 221/250\n",
      "Train Loss: 0.084 | Train Acc: 97.30%\n",
      "Val Loss: 2.038 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 222/250\n",
      "Train Loss: 0.056 | Train Acc: 98.60%\n",
      "Val Loss: 1.994 | Val Acc: 59.39999999999999857891%\n",
      "Epoch 223/250\n",
      "Train Loss: 0.070 | Train Acc: 97.20%\n",
      "Val Loss: 2.046 | Val Acc: 58.79999999999999715783%\n",
      "Epoch 224/250\n",
      "Train Loss: 0.089 | Train Acc: 96.60%\n",
      "Val Loss: 1.886 | Val Acc: 58.19999999999999573674%\n",
      "Epoch 225/250\n",
      "Train Loss: 0.082 | Train Acc: 97.10%\n",
      "Val Loss: 1.910 | Val Acc: 59.00000000000000000000%\n",
      "Epoch 226/250\n",
      "Train Loss: 0.064 | Train Acc: 97.70%\n",
      "Val Loss: 1.927 | Val Acc: 54.40000000000000568434%\n",
      "Epoch 227/250\n",
      "Train Loss: 0.101 | Train Acc: 95.60%\n",
      "Val Loss: 1.949 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 228/250\n",
      "Train Loss: 0.123 | Train Acc: 95.20%\n",
      "Val Loss: 2.041 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 229/250\n",
      "Train Loss: 0.139 | Train Acc: 94.30%\n",
      "Val Loss: 1.838 | Val Acc: 56.99999999999999289457%\n",
      "Epoch 230/250\n",
      "Train Loss: 0.138 | Train Acc: 94.50%\n",
      "Val Loss: 1.840 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 231/250\n",
      "Train Loss: 0.098 | Train Acc: 96.70%\n",
      "Val Loss: 1.903 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 232/250\n",
      "Train Loss: 0.106 | Train Acc: 95.60%\n",
      "Val Loss: 1.963 | Val Acc: 55.40000000000000568434%\n",
      "Epoch 233/250\n",
      "Train Loss: 0.125 | Train Acc: 95.90%\n",
      "Val Loss: 1.953 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 234/250\n",
      "Train Loss: 0.146 | Train Acc: 94.90%\n",
      "Val Loss: 1.925 | Val Acc: 52.80000000000000426326%\n",
      "Epoch 235/250\n",
      "Train Loss: 0.120 | Train Acc: 95.30%\n",
      "Val Loss: 1.772 | Val Acc: 56.00000000000000710543%\n",
      "Epoch 236/250\n",
      "Train Loss: 0.103 | Train Acc: 96.10%\n",
      "Val Loss: 1.876 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 237/250\n",
      "Train Loss: 0.104 | Train Acc: 96.50%\n",
      "Val Loss: 1.844 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 238/250\n",
      "Train Loss: 0.104 | Train Acc: 96.20%\n",
      "Val Loss: 1.908 | Val Acc: 57.19999999999999573674%\n",
      "Epoch 239/250\n",
      "Train Loss: 0.104 | Train Acc: 95.90%\n",
      "Val Loss: 1.919 | Val Acc: 56.39999999999999147349%\n",
      "Epoch 240/250\n",
      "Train Loss: 0.115 | Train Acc: 95.50%\n",
      "Val Loss: 1.925 | Val Acc: 54.20000000000000284217%\n",
      "Epoch 241/250\n",
      "Train Loss: 0.119 | Train Acc: 96.20%\n",
      "Val Loss: 1.994 | Val Acc: 56.79999999999999715783%\n",
      "Epoch 242/250\n",
      "Train Loss: 0.098 | Train Acc: 96.30%\n",
      "Val Loss: 2.015 | Val Acc: 54.60000000000000142109%\n",
      "Epoch 243/250\n",
      "Train Loss: 0.111 | Train Acc: 96.00%\n",
      "Val Loss: 1.967 | Val Acc: 54.00000000000000000000%\n",
      "Epoch 244/250\n",
      "Train Loss: 0.107 | Train Acc: 96.30%\n",
      "Val Loss: 1.828 | Val Acc: 55.60000000000000852651%\n",
      "Epoch 245/250\n",
      "Train Loss: 0.105 | Train Acc: 95.70%\n",
      "Val Loss: 1.935 | Val Acc: 55.80000000000000426326%\n",
      "Epoch 246/250\n",
      "Train Loss: 0.100 | Train Acc: 96.00%\n",
      "Val Loss: 1.908 | Val Acc: 55.20000000000000284217%\n",
      "Epoch 247/250\n",
      "Train Loss: 0.080 | Train Acc: 97.30%\n",
      "Val Loss: 1.898 | Val Acc: 57.79999999999999715783%\n",
      "Epoch 248/250\n",
      "Train Loss: 0.089 | Train Acc: 96.20%\n",
      "Val Loss: 1.877 | Val Acc: 56.20000000000000284217%\n",
      "Epoch 249/250\n",
      "Train Loss: 0.081 | Train Acc: 97.10%\n",
      "Val Loss: 1.928 | Val Acc: 57.59999999999999431566%\n",
      "Epoch 250/250\n",
      "Train Loss: 0.095 | Train Acc: 96.90%\n",
      "Val Loss: 1.957 | Val Acc: 55.60000000000000852651%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/NLP/SW05/wandb/run-20240321_225035-r8hr7uyv/files/best_model.pt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_function, device) \n",
    "    valid_loss, valid_acc = evaluate(model, val_loader, loss_function, device) \n",
    "\n",
    "    # Checkpointing\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"best_valid_acc\": max(valid_acc, wandb.run.summary.get(\"best_valid_acc\", 0))\n",
    "    })\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{config.epochs}')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'Val Loss: {valid_loss:.3f} | Val Acc: {valid_acc*100:.20f}%')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(best_model_wts, 'best_model.pt')\n",
    "wandb.save('best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test batches: 8\n",
      "{'input_ids': tensor([[ 3, 13, 13,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 12, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 8, 22, 27,  4, 26, 26,  4, 27, 22,  8],\n",
      "        [17, 22, 22, 22, 22, 17,  0,  0,  0,  0],\n",
      "        [21,  4,  4, 21,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 18, 22, 22, 18, 24,  0,  0,  0,  0],\n",
      "        [18, 26, 18, 26,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  7, 12, 12,  7,  3,  4,  0,  0,  0],\n",
      "        [ 2, 25, 14, 14, 25, 13, 13,  2,  0,  0],\n",
      "        [27, 18,  8, 16, 16, 26, 18, 14,  0,  0],\n",
      "        [ 5, 22,  5, 22,  0,  0,  0,  0,  0,  0],\n",
      "        [16, 12, 12, 16,  0,  0,  0,  0,  0,  0],\n",
      "        [26,  2, 11, 12, 12, 11,  7, 26,  0,  0],\n",
      "        [10, 26, 11, 11, 10, 26,  0,  0,  0,  0],\n",
      "        [13,  5,  5, 13,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 20,  5, 20, 26, 26, 20,  5, 20, 19],\n",
      "        [21,  8, 23, 23,  8, 21,  0,  0,  0,  0],\n",
      "        [ 3, 15,  6,  5,  6,  6, 15,  3,  0,  0],\n",
      "        [14, 26,  3, 21, 21,  3, 26, 14,  0,  0],\n",
      "        [11, 11, 19, 19,  7,  7,  0,  0,  0,  0],\n",
      "        [25,  7, 21, 25,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 15,  5, 25,  6,  6, 25,  5, 15, 24],\n",
      "        [16,  2, 26, 26,  2, 16,  0,  0,  0,  0],\n",
      "        [16, 23, 15, 13, 13, 15, 23, 16,  0,  0],\n",
      "        [10, 27, 13, 17, 17, 13, 27, 24,  0,  0],\n",
      "        [ 3, 14, 11, 14,  3,  5,  7, 11,  7,  5],\n",
      "        [ 3, 14,  4,  7, 10, 10,  7,  4, 14,  3],\n",
      "        [21, 10,  9,  9, 10, 21,  0,  0,  0,  0],\n",
      "        [14, 26,  8,  8, 26, 14,  0,  0,  0,  0],\n",
      "        [19, 18, 26, 26, 18, 19,  0,  0,  0,  0],\n",
      "        [10, 10, 24,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 24, 12, 27, 15, 15, 27, 12, 24,  2],\n",
      "        [15, 15,  3, 21, 11, 21,  3, 15,  0,  0],\n",
      "        [12, 27, 22, 22, 27, 12,  0,  0,  0,  0],\n",
      "        [17,  6, 19, 19, 21,  5,  0,  0,  0,  0],\n",
      "        [14, 24, 24, 14,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5, 18, 12, 12, 18,  5,  0,  0,  0,  0],\n",
      "        [ 4,  3, 21, 21,  3,  4,  0,  0,  0,  0],\n",
      "        [ 9, 15, 12, 19,  5,  5, 19, 12, 15,  9],\n",
      "        [19,  3, 21, 21,  3,  0,  0,  0,  0,  0],\n",
      "        [ 7, 21,  5,  5, 21,  7,  0,  0,  0,  0],\n",
      "        [27, 21,  5, 27, 27,  5, 21,  0,  0,  0],\n",
      "        [16, 19, 16, 19,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 13, 15, 15, 13, 18,  0,  0,  0,  0],\n",
      "        [11, 26, 23, 15, 26, 11,  0,  0,  0,  0],\n",
      "        [18,  8, 16, 16,  8, 18,  0,  0,  0,  0],\n",
      "        [ 3,  7, 16, 18, 18, 16,  7,  3,  0,  0],\n",
      "        [ 7,  6, 13, 13,  6,  7,  0,  0,  0,  0],\n",
      "        [ 8,  8, 11,  5, 12, 12,  5, 11, 17, 17],\n",
      "        [ 4, 23, 11, 12,  3, 11, 23,  4,  0,  0],\n",
      "        [ 7, 18,  7, 18, 15, 15,  0,  0,  0,  0],\n",
      "        [ 7, 11,  8,  8,  8, 26,  8, 26, 11,  7],\n",
      "        [20,  9, 23, 16, 22, 22, 16, 23, 11,  7],\n",
      "        [18, 13,  7,  7, 13,  4, 18,  0,  0,  0],\n",
      "        [ 9, 12, 10, 14, 14,  6, 10,  9,  0,  0],\n",
      "        [ 4,  9,  9, 17, 10, 23, 17, 10, 23,  4],\n",
      "        [25, 25,  4,  4,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 27, 26, 19, 17, 17, 19, 26, 27, 10],\n",
      "        [ 5, 19, 18, 20,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 10,  4,  2,  2,  4, 10, 11,  0,  0],\n",
      "        [22, 17,  6,  3,  3,  6, 17, 22,  0,  0],\n",
      "        [27, 11, 27,  3,  3, 27, 11, 27,  0,  0],\n",
      "        [21,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [26, 21, 16, 12, 12, 26, 21, 16,  0,  0]]), 'labels': tensor([1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.]), 'sequence_lengths': tensor([ 4,  3, 10,  6,  4,  6,  4,  7,  8,  8,  4,  4,  8,  6,  4, 10,  6,  8,\n",
      "         8,  6,  4, 10,  6,  8,  8, 10, 10,  6,  6,  6,  3, 10,  8,  6,  6,  4,\n",
      "         6,  6, 10,  5,  6,  7,  4,  6,  6,  6,  8,  6, 10,  8,  6, 10, 10,  7,\n",
      "         8, 10,  4, 10,  4,  8,  8,  8,  2,  8])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7760/2804166533.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in input_ids], batch_first=True, padding_value=0)\n"
     ]
    }
   ],
   "source": [
    "# Checking test data set\n",
    "print(\"Number of test batches:\", len(test_loader))\n",
    "# Optionally, inspect a single batch\n",
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7760/2804166533.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in input_ids], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.32%\n",
      "Finished testing\n",
      "Test Accuracy: 50.32%\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, device):\n",
    "    print(\"Testing started\")\n",
    "    model.eval()\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            sequence_lengths = batch['sequence_lengths'].to(device)\n",
    "            \n",
    "            output = model(inputs, sequence_lengths)\n",
    "            preds = torch.sigmoid(output).round().cpu().numpy()\n",
    "            \n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds)\n",
    "            \n",
    "    # Calculate metrics such as accuracy\n",
    "    accuracy = (np.array(predicted_labels) == np.array(true_labels)).mean()\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Log the confusion matrix as an image and test accuracy to Weights & Biases\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    plt.show()  # Ensure the plot is shown in the notebook\n",
    "    \n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(fig), \"test_accuracy\": accuracy})\n",
    "\n",
    "    # Print the test accuracy\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(\"Finished testing\")\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "\n",
    "true_labels, predicted_labels = test(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "##### Winogrande\n",
    "\n",
    "My model is predicting all samples as belonging to either all in class 0 or all in class 1, regardless of the true label. As not even the train accuracy improves over time, my model seems not to learn anything useful from the input provided. I tried out multiple learning rates and hidden/embedding layers but still could not manage to improve the outcome. Maybe longer epochs would help but as I have not really seen the validation loss change a lot, this probably would not help too much. A poor choice of random seed can sometimes lead to unfortunate initializations that lead to poor performance, but I also tried different ones here so this is less likely. It could be that the task is still just too complex for a 2-layer RNN and we might need some more layers or better preprocessing approaches. So unfortunately I was only able to get a test accuracy of 50%.\n",
    "\n",
    "\n",
    "##### Anagram\n",
    "Most of my loss function do not decrease but under certain configuration I see the validation and train loss decrease slightly (e.g. 1e-2 and hidden_layers dim as well as embedding_layers dim 128) which is a good sign that the model is at least learning something. Nevertheless my model already strongly overfits to the train set after about 20 epochs. This means that I probably could have taken a lower epoch size than 500 but I kept it as I still saw some improvment in the validation accuracy in later epochs. On my test set I got an accuracy of 50.07% based on my best model. This accuracy is pretty low compared to the validation accuracy and therefore it seems like the model was too overfitted and does not generalize well on unseen data.\n",
    "\n",
    "Some other take-aways: In the beginning I was doing a mistake and removed the seperator token from the vocabulary but later on realized that I should keep these for this task as they are essential. I also started training at the beginning with a fixed vocabulary size of 45 which I roughly estimated but I should have started right at the beginning with the actual length of vocabulary.\n",
    "\n",
    "##### Palindrome\n",
    "For the palindrome set I discovered almost the same things as for the anagram. It also overfits to the train set very early, after about 15 epochs and test accuracy is very low with only 50.32% which is only slightly better than guessing. \n",
    "\n",
    "Further steps for both of these datasets would be to try out more hyperparameter tuning - especially a higher dropout could help with the overfitting. For both data sets more data could also be useful as 1000 training samples is not a lot for such tasks. Maybe I would have to preprocess the data differently and could get better results as well. Also looking again at the model architecture could maybe show what else I can improve apart from removing the padding again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used ChatGPT to help me completing my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
